
\todo{add direct references to uvm ref manual \cite{UVM12}}

\todo{\\
  - "the" UVM\\
  - configDB\\
  - Fig. or Figure\\
  - DUT or DUT
}

\chapter{Introduction} %///////////////////////////////////////////////////////////////////////////////////////////////////////

Computer chips are around us everywhere in our daily lives. They are in our phones, our cars, our computers and even
in our washing machine. The complexity of these integrated circuits (IC) has increased dramatically since their
inception in the 1960s. While the first ICs consisted of hand-placed transistors, modern hardware designs are
captured in hardware description languages and synthesized by tools to generate the actual layout of the chip. The
scaling mechanisms of Moore's law have given designers an ever increasing number of transistors to work with, which
has led to an increase in features in the designs. With the advent of Systems-on-Chip, a single chip is now composed
of a multitude of complex subsystems each with their own complex functionality.

Producing these chips is costly, with each production run costing millions of dollars. It is therefore imperative to
guarantee that a design will work once a production run is started. This means that the design has to be demonstrated
to be correct before it is sent off for fabrication. In addition to checking for physical correctness in the layout,
the design most importantly also needs to demonstrate functional correctness. This is a challenging task. The
so-called verification of the design can take up to 40-50\% of the total design cost \cite{mehta2018asic}, 70\% of
the design effort \cite{bergeron2012writing} and can manifest up to 80\% of the project code base \cite{bergeron2012writing}.

Considering these numbers, it becomes apparent that verification is a bottleneck in the product development process.
Even worse, verification itself does not add any value to a product, but it is still necessary to ensure that a can
be sold successfully. In order to limit the impact of verification on the overall design cost and time, methods to
make the task more efficient and increase productivity are needed. This means that the effort required to demonstrate
that a feature works correctly should be minimized.

Part of the verification task is to identify and decide exactly how each feature can be demonstrated or proven to
work correctly. This either requires the formulation of properties which can be checked or the development of
scenarios which exercise a certain feature. The other part of the verification task is to implement these test case
scenarios in so-called testbenches where the design is simulated and interacted with in a controlled environment. The
first part requires working with the specification of the design and is hard to automate and make more efficient. The
second part however, can be made more efficient in two ways. On the one hand, productivity can be boosted through the
use of a language which is efficient in expressing the intent of the verification engineer through its language
constructs and features. On the other hand, the amount of code which has to be written can be reduced by creating
flexible components which can be reused and shared across test cases and even projects.

SystemVerilog is the state-of-the-art language for verification. It provides direct support in the language for many
of the features which modern verification techniques rely on. The Universal Verification Method (UVM) is an IEEE
standard for how verification infrastructure should be designed in SystemVerilog to facilitate reuse and
composability. SystemVerilog as a language and UVM as the guiding methodology have found widespread adoption with
numbers from 2020 reporting over 70\% of IC projects using the UVM for verification \cite{foster2020wilson}.
Together, SystemVerilog and the UVM provide a toolset to increase the productivity of the verification task.

However, verification is a software discipline at its core. While SystemVerilog's features such as object-oriented
programming are intended to match the capabilities of modern general-purpose programming languages, efforts exist to
bring verification to more popular and powerful languages to increase productivity and ease adoption. One such effort
is Cocotb, which allows for the use of Python to write testbenches and exploits Python's ease of use and powerful
ecosystem. Other efforts surround the Chisel hardware construction language, where the ChiselVerify project worked on
providing verification features found in SystemVerilog to a Scala verification environment. These efforts however,
focus on providing the primitive tools for verification as they are found in SystemVerilog. They do not answer the
question of how exactly one should use the more powerful languages to write complex and reusable testbenches. While a
UVM implementation exists in Python, PyUVM, it is mostly a direct port of the UVM to Python and does not reevaluate
the design choices of the UVM in the context of Python, except for some minor API adjustments.

This thesis aims to investigate how testbenches should be designed in a modern high-level language environment such
that they are composable, flexible and reusable. The goal is to provide a framework which provides the primitives to
construct testbenches in a standardized manner which takes advantage of the features of the host language.
State-of-the-art verification techniques and languages will be investigated to understand the general concepts and
needs of a verification framework. To gain further insight into verification practices and the adoption of the UVM in
the industry, interviews will be conducted with three companies which are active in the field of IC design and
verification. The insight collected through the background research and the interviews will be used to design a
verification framework. An implementation attempt will be made in a language of choice to demonstrate the feasibility
of the framework.

\todo{how is the thesis structured?}

\begin{comment}

- complexity of design has increased significantly ever since the ..
- while in the first chips transistors were hand-placed, modern design use hardware description langs to describe the behavior and structure of a design and a synthesis tool to generate the actual layout
- the scaling mechanisms of moores law gave designers more and more transistors to work with, which added more and more functionality to the designs
- producing a chips is costly, so they need to be designed to be a first time success
- this means the design has to be demonstrated to be correct before it is sent to the fab
- this includes checking for design rules to be observed in the actual layout
- but most importantly the design has to demonstrate functional correctness
- this is a challenging task, which surpasses the design task itself
- numbers such as 40-50\% of the design cost \cite{mehta2018asic}, 70\% of the design effort and manifest 80\% of codebase \cite[Ch. 1]{bergeron2012writing} show this clearly

- considering these numbers it becomes apparent that verification is a bottleneck in the product development process
- even worse verification does not add value to a product but inversly no verification can lead to a product not being able to be sold

- in order to limit its impact on the overall design cost, verification has to be efficient
- this means less effort per feature demonstrated to work correctly
- specification describes features -> but how do we demonstrate that a feature works correctly?
  - this requires scenarios to be developed or properties to be formulated
  - this constitutes the verification plan
- the verification plan then is used to implement test cases which exercise the design in a controlled environment to demonstrate that the feature works correctly
- implementing these test cases can be made more efficient in two ways
- on the one hand productivity boost can be achieved by language which makes it easy to express the intent of the verification engineer
- on the other hand the amount of code to be written can be reduced by creating flexible components which may be reused and shared across test cases and even projects

- SystemVerilog is the state-of-the-art language for verification
- it provides many of the features which the development of modern verification techniques rely on
- on top of SystemVerilog a IEEE standard for how the testing infrastructure should be designed with a focus on composability and reusability has been developed, the universal verification methodology (UVM)
- these two have become a widely adopted industry standard with 2020 numbers showing over 70\% of IC/ASIC projects using UVM as a methodology \cite{foster2020wilson}

- while UVM is an open standard, it has been influenced from a number of predecessors methdologies developed by the EDA vendors to guide users in the adoption of SV for verification
- non-industry projects focus on basic verification functionality over methodologies
- notably Cocotb provides a python verification framework
- and ChiselTest and ChiselVerify provides scala verification eco system for Chisel designs
- one notable exception is PyUVM but that one is mostly a reimplementation of UVM in python

- UVM was designed for SV
- some parts are language agnostic such as general concepts
- others are limited by the language capabilities
- others again are made possible by the language capabilities

- recent work has focussed on bringing verification to more modern languages using open-source tools
- Cocotb is a python verification framework which allows for the use of python to write testbenches
- library ecosystem mostly matches capabilities of SV
- PyUVM is a reimplementation of UVM in python

- also development surrounding the Chisel hardware construction language
- Chiseltest provides very basic testing infrastructure
- chiselverify worked on providing more complex verification features to match SV capabilities

- Cocotb and Chiselverify do not answer the question of how to acctually design complex reusable testbenches
- they only provide the fabric
- PyUVM only makes slight adjustments to UVM

- so none answers the question of how we can use these modern high level flexbile languages with their many features to construct efficient and reusable testbenches
- this is the question which this thesis aims to answer
- it will investigate state-of-the art verification techniques and tools to understand the general concepts and needs
- to gain further insight into the adoption of UVM and how it is used in practice interviews will be conducted with industry professionals
- this insight will be used to create a verification framework which provides primitives to implement reusable and composable testbenches in a modern language environment

- the thesis is structured in the following way

- present topic
- present problem statement
- why is this relevant
- this is an explorative and problemsolving thesis
- explain the goal/objectives
- what methods will be used
- how is the thesis structured

- verification has a hard time keeping up with increasing design complexity
- verification is a bottleneck in the product development process (40-50\% of design cost) \cite{mehta2018asic}
- UVM is the most used verification framework
- UVM is overly complex and only a subset is actually used \cite{sutherland2015uvm}
- time to reevaluate verification tools

- 70\% of design effort is used on verification \cite[Ch. 1]{bergeron2012writing}
- testbenches manifest up to 80\% of code base \cite[Ch. 1]{bergeron2012writing}

- verification is important to make ASIC design a first time success \cite[Ch. 3]{bergeron2012writing}

\end{comment}

\chapter{Background and Related Work} %/////////////////////////////////////////////////////////////////////////////////////////
\label{ch:background}

In this chapter, the problem of verification itself will be discussed. Furthermore, current approaches to
verification will be outlined and concrete state-of-the-art verification languages and frameworks will be introduced.
The goal is to provide an overview which lays the foundation for understanding what a verification framework should
provide and for analyzing which aspects of the state-of-the-art can be improved.

\begin{comment}

  Verification concerns itself with
hardware, but is itself a software discipline. As such, a framework for verification must concern itself with
software engineering aspects, too. \todo{More specifically, questions of how software is kept modular and
maintainable, }. A selection of these so-called software design patterns will be discussed in the final part of the chapter
- what is the purpose of verification
- what tools do we have to ease verification
- what tools do we have to measure verification progress
- do not introduce "things" but introduce the problems/challenges they are trying to solve

- in this section a complete picture of what verification includes should be drawn in order to put the foundation for
understanding what a verification framework should provide

- to understand current state of verification, we need to look at general approaches, the langauges and their
features, frameworks, as well as methodologies
\end{comment}

\section{Verification} %=======================================================================================================

Verifying a hardware design is about showing that everything works as expected. \citeauthor{bergeron2012writing}
provides his definition as: \textit{``Verification is a process used to demonstrate the functional correctness of a
design''} \cite[Ch. 1]{bergeron2012writing}.

On the surface the task seems well-defined and straight-forward. However, looking closer, a series of challenges
become apparent.The specification, unless captured in a formal language, is open to interpretation. A
misunderstanding could lead to a wrong implementation. The likelihood of catching this can be increased by
letting a different engineers implement and test the implementation, but it can not be reduced to zero \cite[Ch.
1]{bergeron2012writing}.

Unless a formal language is used for the specification, it is also impossible to \textit{prove} that an
implementation matches the specification. This is why \citeauthor{bergeron2012writing} uses the word
\textit{demonstrate} instead. Verification is thus question of confidence and about convincing oneself that the
implementation matches the intent captured in the specification. This confidence is built by demonstrating that the
implementation behaves as the specification prescribes in a selection of scenarios \cite[Ch. 1]{bergeron2012writing}.

These scenarios are captured in a verification plan. It holds a list of features and a set of conditions under which
a certain behavior should be observed. In short, it defines a series of test cases for each feature, which by itself
should demonstrate that the feature works correctly. Whether all edge-cases have been captured in these
demonstrations is another question, which the verification team has to convince themselves of through a thorough
process \cite[Ch. 1]{bergeron2012writing}.

The fact that correctness can not be proven based on a non-formal specification can not be helped by using model checking
methods either, which can prove that a certain property holds for a given implementation. The properties themselves
are an interpretation of the specification and thus subject to the same ambiguities and misunderstandings \cite[Ch.
1]{bergeron2012writing}.

The tool by which correct behavior of a feature is demonstrated is the \textit{testbench} in a process referred to as
\textit{functional verification}. Functional verification focuses on design intent and that the implementation
provides certain functionality. The testbench is a closed system which tries to emulate the environment of the design
in a controlled manner. It exercises the design such that it becomes evident, by manual or automatic inspection, that
a feature is working as intended \cite[Ch. 1]{bergeron2012writing}.

\begin{comment}

\cite[Ch. 1]{bergeron2012writing}
- important question is: what is being verified?
- the answer lies in the specification
- how precise is the specification, are there ambiguities?
- how complete is the specification, are there corner cases not covered?
- unless the spec is captured in a formal languages, it is impossible to prove that an implementation matches the spec
- verification is thus a process of convincing oneself that the implementation is correct beyond a reasonable doubt

- the spec itself is not used though, it is an interpretation of the spec by an engineer which is used to design an implementation but also the testbench
- important for these two processes to be performed by different people, such that there is at least a chance of discovering misinterpretations

- the problem is inherent in the transformation of a specification
- also not fixed by model checking methods, where properties are still DERIVED from spec

\cite[Ch. 1]{bergeron2005verification}
- progress is measured in number of features demonstrated to work correctly

\cite[Ch. 1]{bergeron2012writing}
- functional verification focuses on demonstrating that a certain functionality meets the intent captured in the specification
- in order to do this, the design is exercised in a controlled environment, the testbench
- it exercises the design such that it becomes evident, by manual or automatic inspection, that a feature is working as intended
- testbench itself is a close system which tries to emulate the environment of the design in a controlled way

\cite[Ch. 1]{bergeron2005writing}
- functional testing can be differentiated by the degree on which signals internal to the DUT are used to determine the correctness of the design
- problem lies in controllability and observability of the DUT
- how easy is it to trigger certain behavior, how easy is to it to see that the behavior was executed correctly
- black-box testing only uses the inputs and outputs of the DUT, can especially suffer from low observability, since an potential error has to be detected far away from the actual cause
- other extreme is white-box testing where full visibility of the internals is used in the test, it suffers from its lack of generality -> a change in the DUT requires a change in the testbench
- a compromise between both can be referred to as grey-box testing, tries to limit dependency on implementation specific stuff while still providing good observability and controllability

\end{comment}

\subsection{Functional Verification} %-----------------------------------------------------------------------------------

% also talk about assertions

Functional verification can be differentiated by the degree on which signals internal to the device under
test (DUT) are used to determine the correctness of the design. From a maintainability perspective, it makes
most sense to defer from accessing any internal signals in the implementation as part of the testbench and seeing the
DUT as a black-box with a certain interface. Changes in the implementation would in this case require no changes in
the testbench. It also means that different implementation attempts sharing the same interface could reuse the same
testbench. However, this approach can suffer from a low controllability and observability. Controllability refers to
how easily a certain functionality can be triggered inside the DUT by driving its inputs. Observability on the other
hand, refers to how easily the effect of a certain functionality being triggered becomes visible at the outputs of
the DUT \cite[Ch. 1]{bergeron2012writing}.

In complex designs some features may suffer from low controllability and/or observability. In this case, white-box
testing can be used, which provides full access to the internals of the DUT. Register state as well as outputs of
functional units related to the functionality under test can be directly accessed to verify its correctness. This
approach suffers from the fact that each change in the implementation may require a change in the testbench. A
compromise between black-box and white-box testing is grey-box testing. It tries to balance the dependency on
implementation details with maximizing observability and controllability \cite[Ch. 1]{bergeron2012writing}.

Functional verification using testbenches and test cases can be aided by assertions, a way to check if a property
holds in the implementation. Assertions can for instance be used to check if a certain condition holds at a certain
point in time, over a certain period of time or when another condition holds. They are placed directly in the source
code next to the functionality that they are checking a property of. This gives them maximum visibility leading to
maximum observability. These assertions can be used alongside a normal testbench which applies stimulus to the DUT
though its interfaces. Should a property specified in an assertion at any point not hold, the error will point
directly to the source of the problem. This is in contrast to a traditional testbench where an error occurring on the
interface has to be traced back to the actual cause \cite[Ch. 14]{mehta2021introduction}.

Up until now, only dynamic functional verification has been discussed, where the behavior of the implemented design
is observed by \textit{simulating} it. This is in contrast to static functional or \textit{formal} verification,
where a tool will try to disprove the property by analyzing the logic of the design itself. If the tool is able to
disprove the property, it will also have a counter-example which shows how the property can be violated. If the tools
is not able to disprove the property, it is guaranteed to hold under any condition which the design can be exposed to
\cite[Ch. 14]{mehta2021introduction}.

This can obviously be a powerful tool for finding bugs in an implementation. The limitations however are twofold.
Properties are effectively small formal models of the functionality under test and are hard to develop. Also, the
performance of analyzing a property deteriorates quickly with the complexity of the design. The process suffers from
the so-called "state space explosion" problem. The tool has to analyze how applying a certain set of inputs will
develop cause future state transitions. In a complex design, the number of possible choices which a system has,
leading to new choices and so on, just becomes to large to handle within reasonable time. As a solution, hybrid
approaches exist, where static methods are combined with dynamic ones. The design is lead to a certain state in a
simulation and then it is statically checked whether the property can be violated from this state \cite[Ch.
14]{mehta2021introduction}.

\subsection{Constrained-Random Verification} %---------------------------------------------------------------------------------

With ever increasing design complexity, it is not feasible to write directed test cases to methodically exercise and
verify each feature under each possible condition as outlined in the verification plan. Instead, randomized stimulus
can provide a way to increase productivity. If a long enough steam of random stimulus is applied to the interfaces of
the DUT, it is likely that many of the test cases outlined in the verification plan will occur naturally, without any
additional effort. It is even possible that a random stream of stimulus may create unforeseen conditions, improving
the thoroughness of the verification effort \cite[Ch. 1]{bergeron2005verification}.

Of course, it does not make sense to apply completely random stimulus to the DUT interfaces. Only a fraction of the
applied stimulus would often be valid for complex interfaces like ethernet or PCIe ports. A mechanism to define what
a valid transaction for a given interface looks like is needed. This is achieved by constraining the random stimulus.
Constraints limit the set of legal assignments to the set of random variables used to derive a random stimulus for a
given interface \cite[Ch. 3]{bergeron2012writing}. These constraints could for instance capture that the length field
of an ethernet frame has to match the actual length of the payload, but also that the address in a bus transaction
has to be aligned to the size of the transfer.

The verification runtime will, given the constraints on the random variables, generate a set of assignments which
satisfies all constraints using a constraint satisfaction problem solver. In addition to inter-variable
constraints, distribution constraints are highly relevant,
in order to ensure that the random stimulus is representative of the actual behavior of the DUT \cite[Sec. 7.5]{flake2020a}.

The verification approach of using random stimulus alongside constraints is called \todo{constrained-random verification}
(CRV). It promises an increase in productivity, which is given by a single CRV testbench being able to exercise
potentially many of the test cases outlined in the verification plan. This allows for an approach where verification
is started off using CRV, and only then more specific tests are developed to target functionality which is difficult
to exercise using CRV \cite[Ch. 3]{bergeron2012writing}. The question of couse is, how to measure what has been
tested and what has not.

\begin{comment}

start of with CRV and then add specific tests with constrains for uncovered features \cite[Ch. 3]{bergeron2012writing}

by exercising many of
these features with a single, very long stream of random stimulus . If the stimulus
generator is properly constrained to produce valid transaction patterns, it may not only produce the test cases
captured in the verification plan, but may even lead to conditions not considered in the verification plan, like some
hard-to-trigger edge cases \cite[Ch. 3]{bergeron2012writing}.

\cite[Ch. 13]{mehta2021introduction}

- idea is quite simple
- but achieving actually exhaustive random stimulus can be difficult

\end{comment}

\subsection{Coverage} %--------------------------------------------------------------------------------------------------------

A testbench is measuring how well the DUT lives up to the specification. But how well is the collection of
testbenches living up to checking all aspects outlined in the specification? In a case, where only directed tests are
used, the answer is simple since each test case can be traced back to a specific feature in the specification. When
CRV is used on the other hand, it becomes essential to have some kind of tool which can actually measure which
functionality has been exercised and which has not \cite[Ch. 15]{mehta2021introduction}.

One tool which can give an idea about how thoroughly a design has been tested is code coverage. It provides insight
into how much of the source code, structurally but also semantically, has been exercised by a test suite. By adding
instrumentation to the design, the activity of certain aspects of the design can be monitored while running a
testbench \cite[Ch. 2]{bergeron2012writing}.

At the most basic level, line coverage checks whether a line of code has been executed. More useful is statement
coverage which considers each individual statement independent of its position in the code. A statement could for
instance be an assignment operation. Branch coverage concerns itself with control flow such as if-statements,
measuring how many of the different paths through a piece of code have been taken. Finally, finite-state machine
(FSM) coverage recognizes FSM patterns in the source code. It can report which states of the FSM have been visited
and which transitions have occurred. FSM coverage does not know about valid transitions though and as such all
possible transitions are reported. Illegal ones have to be filtered out manually to extract a useful metric out of
FSM coverage data \cite[Ch. 15]{mehta2021introduction}.

Code coverage however, does not know anything about the functionality of the design. A low code coverage indicates that
some functionality of the design has not been exercised and thus tested. A 100\% code coverage on the other hand does
not mean that the verification task is done \cite[Ch. 2]{bergeron2012writing}.

To measure the coverage of a test suite at the functional level, a more general tool is needed. This so-called
functional coverage is manifestation of the verification plan and thus of the design specification, tracking whether
each functionality has been exercised under all relevant conditions \cite[Sec. 7.6]{flake2020a}. It can not be
automatically derived from a non-formal specification and thus has to be manually defined \cite[Ch. 15]{mehta2021introduction}.

The idea behind functional coverage is to monitor state, transitions, changes to variables or expressions and a
combination of these (so-called cross-coverage). For each functionality, different cases of interest have to be
defined and declared as so-called bins. Signals or variables can have different bins for specific values or ranges
which may be augmented with predicates. At a given sampling event these bins record whether they have been hit, i.e.
whether the condition they represent has been observed. Cross-coverage creates the Cartesian product of the bins
associated with two variables and records when combinations of bins have hits at the same time. The goal is to have
all bins hit at least once, indicating
that all aspects of the design have been exercised \cite[Sec. 7.6]{flake2020a}.

\begin{comment}

\cite[Ch. 15]{mehta2021introduction}
- how to measure how well the testbench is performing?
- especially in the case of CRV this becomes essential, since it is not clear what functionality has been tested under which conditions

\cite[Ch. 2]{bergeron2012writing}
- code coverage provides insight into how much of the source code or structure of the DUT has been exercised by a test suite
- via adding instrumentation to the design, the activity of certain aspects of the design is monitored

\cite[Ch. 15]{mehta2021introduction}
- at the simplest level, line coverage checks whether a line of code has been executed
- more useful is statement coverage which looks at each statement independent of its position in the code
- branch coverage concerns itself with control flow, it measures how many of the different paths through a piece of code have been taken
- finally, FSM coverage recognizes FSM patterns in the source code
- it can report which states have been visited, which transitions have occurred
- does not know about valid transitions though, illegal ones have to be filtered out manually

- code coverage can indicate that the verification is NOT down, never whether it is done \cite[Ch. 2]{bergeron2012writing}

- a more general concept of coverage is needed, functional coverage \cite[Sec. 7]{flake2020a}
- it has to be based on the design specification, can not be derived and thus manual work \cite[Ch. 15]{mehta2021introduction}

\cite[Sec. 7]{flake2020a}
- essentially about monitoring state, state transitions, changes to variables and expressions and combinations of these (cross)
- one bin for each state, transition, cross... which corresponds to a functional aspect of the DUT
- all bins should have hits

\end{comment}

\subsection{Testbench Abstractions and Reusability} %--------------------------------------------------------------------------

Writing monolithic testbenches to demonstrate the correctness of each feature
in a design is not an efficient and sustainable solution. The testbenches would become too complex and large to
maintain, and a lot of development time would be
used to code some of the foundations each testbench relies on. A testbench can be split into two parts at the highest
level: the part which eases the interaction with the DUT programmatically, the so-called test harness, and the part
which is specific to the test case \cite[Ch. 6]{bergeron2012writing}.

If these two parts were to be split such that the test harness could be reused across all testbenches for a given
DUT, the question of what interface the test harness exposes to the test case code arises. Since no abstraction is
achieved by working with the ports of the DUT directly, a higher level of abstraction is needed. This is where the
concept of transactions comes in. A transaction is an operation on an interface, as abstract as the transmission of a
TCP package but also low level like an AXI bus write operation \cite[Ch. 1]{bergeron2005verification}.

Given the abstraction level of transactions, components are needed to bridge the gap between different levels of
abstractions. These components are called transactors. Since the gap between the highest level of
abstraction and actual pin interactions can be quite large, such as in the case of TCP packages, intermediate levels
of abstraction should be introduced, with transactors bridging the gap between them. This means that transactors can
be layered and composed to meet the needs of a specific test case \cite[Ch. 4]{bergeron2012writing}.

The lowest levels of transactors, which interfaces the pins of the DUT, are called bus-functional models (BFM). In
contrast to other transactors, BFMs are concerned with timing \cite[Ch. 4]{bergeron2012writing}. They encapsulate the
protocols of an interface and translate a transaction into a potentially multi-clock cycle interaction with the
interface pins of the DUT \cite[Ch. 3]{salemi2013uvm}.

Transactors assume that the DUT correctly handles all interactions of the abstraction levels below it. As such, a
test should exist, verifying that each level of abstraction is correctly handled by the DUT. For instance, in a
packet based system with multi-cycle transmission, one test should verify that parts of a packet are correctly sent,
such that all other test can safely assume this property \cite[Ch. 6]{bergeron2012writing}. This methodology centered
around transactions and transactors allows for the structuring of a testbench into reusable components, which can be
composed to meet the needs of a specific test case and thus can increase the test case development productivity.

\begin{comment}

\cite[Ch. 1]{bergeron2005verification}
- transaction is an operation on an interface, as abstract as the transmission of a TCP package but also low level like an AXI bus write operation

\cite[Ch. 6]{bergeron2012writing}
- tb has two parts test harness and test case specific code
- harness is possible to reuse

\cite[Ch. 3]{salemi2013uvm}
- BFM concerned with signal level -> first step towards transactions
- BFM encapsulates the protocols of an interface
- instead of driving signals, call methods which advance sim time while driving pins
- in SystemVerilog this is implemented using the interface construct

\cite[Ch. 6]{bergeron2012writing}
- BFMs can be layered, where a higher level BFM calls a lower level BFM
- each BFM assumes correctness of some interaction with the DUT
- this can be demonstrated in one separate test such that all others can rely on this

\end{comment}

\section{Verification Languages and frameworks} %===============================================================================

Having considered the general problem and concept of verification, the focus will now shift to actual frameworks and
languages which are used to perform verification with. First, a look will be taken at the SystemVerilog language and
its predecessors to better understand its design choices. Then, the Universal Verification Methodology (UVM) will be
discussed in detail. Finally, open-source alternatives to SystemVerilog and UVM for verification will be introduced.

\subsection{SystemVerilog's Predecessors} %------------------------------------------------------------------------------------

In the mid 1990s, some of Verilog's limitations became apparent as the complexity of designs grew significantly. Next
to movements trying to replace HDLs altogether with general-purpose programming languages through high-level
synthesis, others tried to strengthen Verilog by making it more capable especially in terms of its specification and
verification features \cite[Sec. 6]{flake2020a}.

One such attempt was Superlog. A team at Co-Design Automation Inc. thought that Verilog lacked features of
general-purpose languages to be efficient for verification. Being focused preserving the high performance of Verilog
simulators, the group turned to C for inspiration. The resulting Superlog language added C data types like enums and
structs, dynamic memory allocation, but also dynamic data types likes strings, dynamic arrays and queues. Another
reason to draw inspiration from C was to make C code and Superlog easily interoperable which was useful for
integrating C models in verification code \cite[Sec. 6]{flake2020a}.

Another addition over Verilog was the introduction of interfaces, which on the one hand allow bundling of signals
with directionality, and on the other hand allow for exposing functions from one module to another. The latter
feature was deemed useful for transaction level models, allowing Superlog to be used as a specification language
\cite[Sec. 6]{flake2020a}.

Another direction of development focused only on the verification aspects and lead to the development of hardware
verification languages (HVL). These were languages specifically tailored to the task of verifying hardware designs,
adjusting the level of abstraction and the expressiveness of the language to needs of the verification tasks. One of
these languages was Vera, initially developed at Sun Microsystems in 1994 \cite[Sec. 7]{flake2020a}.

The Vera language was built around a co-simulation engine which could interface different hardware simulators, and
was meant to be HDL agnostic. The level of abstraction in interacting with the DUT is slightly lifted, by removing
the need for explicit timing and focusing instead on clock cycle based interaction. The IO of the DUT is captured in
interfaces which are associated with a clock, outputs defining a skew relative to a clock edge when they should be
driven and inputs defining a skew relative to a clock edge where they should be sampled. These interfaces can be
bound to ports which in turn can be passed around the testbench to provide access to the DUT. Signals can be directly
referenced and assigned in the code, with the runtime taking care of the actual timing in the simulation running in
the background \cite[Sec. 7]{flake2020a}.

Vera offers flexibility in scheduling interactions with the DUT through the \ttt{\@} operator, which allows to defer
assignments and assertions to future clock cycles or synchronize the current thread with a signal change. Assertion
have a timing dimension, e.g. allowing to specify intervals in which the condition should become true or continuously
hold . A Vera program starts in a single thread, from which it can spawn new threads using a fork-join model.
Multi-threading constructs such as synchronization events, semaphores and mailboxes are part of the language
\cite[Sec. 7]{flake2020a}.

One of the maybe most important features added by Vera is the inclusion of object-oriented programming (OOP). Vera
introduces classes and inheritance, which allows for a more structured and modular testbench design. Equally as
important are the addition of constrained randomization and functional coverage collection. Class fields in very can
be marked with the rand keyword. A call to the \ttt{randomize} method on an object of the class will then randomize
all fields marked with rand according to the constraints specified in the class. Functional coverage can be collected
by defining bins for states, transitions and general expressions or the cross-product of these. A later addition to
the Vera language when it was adopted as an open standard by Accellera as OpenVera, was the introduction of Open Vera
Assertions (OVA). These rely on linear temporal logic, a type of formal logic used to describe sequences of events
over time, which allows to capture highly complex behavior in assertions \cite[Sec. 7]{flake2020a}.

\begin{comment}

- mid 1990s Verilogs limitations became apparent as design complexity grew
- discussed replacing HDLs altogether with C++ or java for hardware design
- movement to strengthen Verilog lang -> Superlog
- verilog lacked features of general purpose programming language to be efficient for verification
- Superlog mostly superset of verilog with inspiration from C for data types like enums and structs and primitives ike strings and arrays, queues, dynamic memory allocation
- easy to interface with C code
- introduces interface construct to bundle signals but also allow exposing functions for transaction level modelling by associating them with an interface, they can operate on the signals of the interface

- other developments focussed only on the verification aspects
- verilog not designed for modular complex testbenches
- not very concise in expressing verification intent
- lead to development of HVLs

- one of these was Vera
- a cosimulation engine interfacing with hardware simulators
- language agnostic
- lifts abstraction level by focussing on steady state behavior
- for this signals are associated with clock and samples relative to that clock
- skews relative to clock edges are provided to define sampling and drive times
- IO of dut is captured in interface which can be bound to ports which can be freely passed around the testbench to provide access to the DUT
- signals can be directly referenced and assigned in the code, the runtime takes care of the actual timing
- interactions can be deferred to future clock cycles using the @ operator
- assertions can be directly written as boolean expressions with a potential interval or point in time when the condition should hold
- synchronization apart from the clock cycle granularity is provided by @(signal) or @(posedge signal)
- program starts from single thread and can spawn new threads using fork-join model
- multi-threading constructs like events for synchronization, mailboxes and semaphores are part of the language
- adds object oriented features like classes and inheritance
- adds crv features by marking class fields with rand keyword and specifying constraints
- finally vera adds functional coverage by allowing to define coervage bins for states, transitions and general expressions
- a later addition to vera was OVA open vera assertions, which relied on LTL to specify complex temporal properties

OpenVera \cite[Sec. 7, pp. 51-??]{flake2020a}

\end{comment}

\subsection{SystemVerilog} %---------------------------------------------------------------------------------------------------

Towards the early 2000s, motions where made to combine the progress which had been made in the development of more
modern hardware design and verification languages like Superlog and Vera into one standardized language. The idea was
to have one language for design and verification, enabling modern verification techniques such as CRV, functional
coverage collection and complex assertions. The result was SystemVerilog which was first publicized as a standard in
2002 by Accellera and after several revisions and rework was made a IEEE standard in 2005 \cite[Sec. 9]{flake2020a}.

SystemVerilog is mostly based on Superlog and Vera, the API of latter only being slightly changed to harmonize the
syntax with the rest of the language. The SystemVerilog subset for assertions was not only based on Vera's OVA, but
also drew inspirations from other specification languages such as the property specification language PSL, an
Accellera and later IEEE standard based on temporal logic. As such SystemVerilog is actually four languages in one
\cite[Ch. 1]{mehta2021introduction}:

\begin{enumerate}
  \item A subset for synthesis
  \item A subset for object-oriented verification
  \item A subset for coverage collection
  \item A subset for assertions
\end{enumerate}

This makes SystemVerilog a complex but powerful language. It allows for the use of one language for all aspects of
the design process, from building models to implementing a synthesizable design to verifying that the design meets
the specification \cite[Ch. 1]{mehta2021introduction}.

\subsection{UVM} %-------------------------------------------------------------------------------------------------------------

The verification features which SystemVerilog offers are by themselves not enough to create reusable testbenches. They provide
the raw mechanisms for creating modern constrained-random, self-checking testbenches with coverage collection and
bus-functional models, but the language itself does not prescribe a way of organizing the different responsibilities
in a testbench such that large testbenches become manageable and such that parts of a testbench can be reused in
another project or be bought from another company. To achieve this, design principles and a methodology are needed for
the construction of testbenches \cite[Sec. 9.2]{flake2020a}.

Acknowledging the problem, each EDA vendor had developed their own reuse methodology: Mentor Graphics had the Open
Verification Methodology (OVM), Cadence had the Universal Reuse Methodology (URM) and Synopsys had the Verification
Methodology Manual (VMM). Due to the fear of vendor lock, none of these methodologies gained widespread adoption
\cite[ch. 4.1]{mehta2018asic}. Under the umbrella of the Accellera Systems Initiative, a standards organization
supported by the EDA industry, a merger of the different methodologies was attempted. The result was the Universal
Verification Methodology (UVM) which is mainly based on the OVM \cite[ch. 4.1]{mehta2018asic}.

The UVM provides standardized ways to create testbench infrastructure which is hierarchical and where
responsibilities are divided over different components. Furthermore, it introduces the concept of phases,
synchronizing components throughout the different steps of a test case. Finally, the UVM raises the level of
abstraction of the testbench by working with transactions which encapsulate a potential multi-clock cycle interaction
with the DUT in one object \cite[ch. 4.1]{mehta2018asic}. In the rest of this section, the core concepts of the UVM
will be introduced. The focus will be primarily on the functionality and ideas. Implementation details will only be
highlighted when they are relevant to the discussion.

\subsubsection{UVM Testbench Structure} %--------------------------------------------------------------------------------------

The UVM offers a separation of concern between three different perspectives: the test writer, the sequence writer and
the environment writer \cite{sutherland2015uvm}. The test writer constructs a test case to exercise a specific
feature by chaining a series of sequences together and applying them to the DUT by handing them to the environment.
He does not need to know how the sequences are constructed or how the DUT is driven. The sequence writer creates
sequences of transactions, so-called sequence items, which achieve a specific goal in the DUT. He does not need to
know how the transactions are
driven onto the DUT either. Only the environment writer concerns themselves with the translation of transactions into
pin-level signals and the driving of these signals onto the DUT. The environment writer also creates the analysis and
checking infrastructure of the testbench like coverage collectors or scoreboards. The environment manifests the
static infrastructure of the testbench which is shared between different test cases. It contains components which
direct the stimulus generated by the test case to the relevant DUT interfaces and transactions observed on the DUT
interfaces to analysis components for coverage
collection or checking against a model \cite{sutherland2015uvm}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{diagrams/uvm_structure.pdf}
  \caption{The hierarchy of a simple UVM testbench.}
  \label{fig:uvm_tb}
\end{figure}

The full UVM testbench hierarchy is presented in Figure \ref{fig:uvm_tb}. As it can be seen, in a complex system, the
top-level environment itself consists of multiple other environments, which are specific for sub-systems of the DUT.
Inside each environment, there are a series of agents which are specific to the interfaces which are part of the
sub-system. Additionally, components collecting coverage information at the sub-system level may be present. Finally,
the environment contains a scoreboard which compares the transactions observed on the DUT interfaces with
transactions produced by some kind of model, the predictor.

An agent is specific to one interface, for instance AXI4 or SPI, and bridges the gap between the transaction level
and the driving and reading of the pins of the DUT. Agents come in two forms: passive and active. Passive agents only
observe transactions on the interface pins and send them to analysis components such as scoreboards or coverage
collectors. Active agents also accept transactions which they drive onto the interface pins. As it is shown in Figure
\ref{fig:uvm_tb}, an active agent consists of a sequencer, a driver and a monitor. A sequencer serializes
transactions from sequences and sends them to the driver, which applies them to the DUT through the use of virtual
interfaces in SystemVerilog. The monitor observes the interface and converts the activity into transactions which are
used by analysis components such as coverage collectors or scoreboards \cite[ch. 4.3]{mehta2018asic}.

\subsubsection{Communication between components} %----------------------------------------------------------------------------

The communication between the different components of the testbench is facilitated by the TLM (Transaction Level
Model) interface. The TLM is built around the idea of push and pull channels. The active side of a channel connection
(the producer in a push channel, the consumer in a pull channel) is called the \ttt{port} while the passive side is
called the \ttt{export}. The \ttt{export} provides a method for handling the receiving or sending of a transaction
which is triggered by the \ttt{port} \cite[ch. 4.5]{mehta2018asic}.

Most of the time, two components are connected with a buffer between them such
that a buffered channel is created. In this case the buffer provides the two passive channel ends such that the
sender can call \ttt{put} at any time and the receiver can call \ttt{get} at any time. Calls to these methods are
blocking when no data is in the buffer. In the testbench, ports can be forwarded from a child component to a parent
component, exposing the port at a higher hierarchy level. This is for instance the case in an active agent, where the
port for the transaction stream to the driver is exposed by the agent itself. Next to single-producer-single-consumer
channels, the TLM also provides one-to-many channels with so-called \ttt{analysis\_port}'s. These are used to
broadcast transactions to multiple analysis components like coverage collectors or scoreboards \cite[ch. 4.5]{mehta2018asic}.

\subsubsection{Phases} %-------------------------------------------------------------------------------------------------------

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{diagrams/phases.pdf}
  \caption{Overview of the UVM phases. Top-down phases are red, bottom-up phases are yellow and the run phases are blue.}
  \label{fig:uvm_phases}
\end{figure}

Another concept introduced by the UVM are phases. Working with more elaborate DUTs with complex reset sequences, it
becomes natural to split the handling resetting the DUT from the actual test case. In a parallel system like UVM
where multiple components run concurrently, the challenge then is how the components should agree when the test case
starts. As another example, analysis components in the testbench need to know when the test case has finished in
order to generate summary statistics \cite[ch. 4.6]{mehta2018asic}.

UVM solves this synchronization issue by defining a series of phases with each component having the option run during
a phase by overriding a specific method. An overview of all UVM phases can be seen in Fig. \ref{fig:uvm_phases}. The
phases can be divided into three groups: build phases, run phases and cleanup phases. In the build phases, the
testbench hierarchy is set up. The \ttt{build} phase runs top-down through the hierarchy. Here all component
instances are constructed using the UVM factory and each component can configure its children by setting the
ConfigDB. The \ttt{connect} phase runs bottom-up and is used to connect the TLM channels between the components
\cite[ch. 4.6]{mehta2018asic}.

In the run phases, the actual test case is run and the DUT is stimulated. That means that these phases are the only
ones consuming simulation time, signified by the usage of \ttt{task}'s instead of \ttt{function}'s in SystemVerilog.
As it can be seen in Fig. \ref{fig:uvm_phases}, next to a series of fine-grained run phases, there is also the
\ttt{run} phase itself which spans the whole simulation. The most important run phases are \ttt{reset},
\ttt{configure}, \ttt{main} and \ttt{shutdown}. During the \ttt{reset} phase, the DUT can be forced into a known
state. The \ttt{configure} phase should be used to set up the DUT for the actual test case, e.g. by setting up
memories. The \ttt{main} phase is where the actual test is performed, i.e. the stimulus is applied to the DUT. The
\ttt{shutdown} phase can be used to wait for all effects of the applied stimulus to take place \cite[ch. 4.6]{mehta2018asic}.

As noted by the authors of \cite{dvcon2014reset}, reset behavior should not only be verified at the start of a test
but potentially also midtest in CRV test cases. The UVM does not directly prescribe a way to handle this, and altough
jumping between phases is possible, this poses the challenge of gracefully terminating the current phase in all
components. The authors propose a reset monitor which invokes cleanup hooks in all components when a reset is detected.

The cleanup phases in the UVM are used for analysis components to determine whether the test was passed successfully
and whether secondary goals such as coverage were met. The purpose of each phase in this group is very narrow and is
evident by the name \cite[ch. 4.6]{mehta2018asic}.

\subsubsection{Transactions} %-----------------------------------------------------------------------------

We have seen that the stimulus for the DUT is abstracted as transactions in the UVM. Transactions are passed around
the testbench through TLM channels, decoupling the actual data in the transactions from the static structure of the
testbench. But transactions rarely come alone. Usually, a series of transactions is needed to test a specific feature
of the DUT. These sequences of transactions should not be produced in the test case itself, since that would not
allow them to be reused across different test cases. Instead the test case should rely on predefined sequences of
transactions which it composes to achieve the desired stimulus \cite[Ch. 23]{salemi2013uvm}.

In the UVM, transactions are derived from the \texttt{uvm\_sequence\_item} base class. A sequence item consists of
fields, some of which can be randomized subject to constraints, which encode a transaction. Sequences of transactions
are derived from the \ttt{uvm\_sequence} base class. The resemble generators, as they can be found in Python or F\#,
where a code body is executed which produces an ordered sequence of items. In Python and F\#, the \ttt{yield} keyword
is used to produce items. In the body of a UVM sequence, the \ttt{start\_item} method synchronizes the sequence with
a potential consumer. A call to \ttt{finish\_item} signals that the item is ready and it is sent to the consumer. A
feature which is not a part of usual generators is, that the consumer can answer itself with a transaction of a
potentially different type to the transaction it received from the generator \cite[Ch. 4.3]{mehta2018asic}.

To maximize reuse, sequences should themselves be composable. Combining sequences should not be done by the test case
itself, since this would again not allow for reuse of the composed sequences. The UVM solves this by allowing for
so-called virtual sequences, which are sequences that "play" other sequences in their body. A test case can reference
such a virtual sequence instead of composing the individual sequences itself. The individual sequences in a virtual
sequences do not have to necessarily target the same consumers, which means that sequences can be sent to different
drivers in parallel. This allows for the encapsulation of cross-interface interactions with the DUT in a single
construct \cite[Ch. 23]{salemi2013uvm}.

One aspect which has not been discussed yet, is how the transactions actually enter the static infrastructure of the
testbench. Instead of being directly sent to a driver, transactions pass through a so-called sequencer in the UVM
before ending in a driver. When a sequence is started, i.e. it starts producing transactions, it is handed a
reference to a sequencer. The sequencer has a TLM port to which it forwards the transactions it receives from the
sequence. A sequencer can possibly receive transactions from multiple sequences at the same time, and it is up to the
implementation to arbitrate the order in which the transactions streams are merged \cite[Ch. 23]{salemi2013uvm}.

Sequences can be composed and run in parallel on multiple interfaces using virtual sequences. In this case the
virtual sequence handles the coordination between the different transaction streams. Sometimes it may however be
necessary to coordinate the transaction streams to multiple interfaces which originate from \textit{different}
sequences. This could for instance be the case, if a timing relationship between handshakes on different interfaces
is required. Here the UVM provides the concept of a virtual sequencer. It holds references to a set of actual
sequencers and provides mechanisms to control them. Sequences, virtual or not, target their sequencers via the
virtual sequencer which coordinates the interactions between the different interfaces \cite{virtualseq}.

\begin{comment}
\cite[Ch. 23]{salemi2013uvm}
- in order to make testbench reusable, structure should be separated from data i.e. Transactions
- this also includes the stimulus that is the ordered sequences of transactions
- the ordered sequences could also be used across different test cases -> test case should not decide the specific transactions
- instead test case could operate on collections of transactions which are used like a library

\cite[Ch. 4.3]{mehta2018asic}
- sequence items are the base class for transactions in the UVM
- just class which consist of random and non-random fields with constraints
- sequences represent a stream of sequence items
- in the code they resemble generators
- a body method is invoked and new sequence items are produced as part of the sequence by calling a method
- a special thing is through that when a sequence item is produced, a response may also be obtained which could be another transaction type
- the start\_item method passes the reference to the sequence item to the receiver and waits until a receiver is ready
- when finish\_item is called, the receiver is informed that the item is valid and can be processed
- the sequence is paused until the receiver signals that it is done with the item and a response item may follow that confirmation

\cite[Ch. 23]{salemi2013uvm}
- but what if we want to compose sequences such that the combination is reusable
- can not be done in the test case
- instead a virtual sequence can be used
- it can play other sequences in its body
- sequences can be played in parallel and may be sent to different targets
- this gives control on how interactions on different interfaces are interleaved

\end{comment}

\subsubsection{Component Types} %----------------------------------------------------------------------------------------------

Now that the high level structure and way of functioning of a UVM testbench has been introduced, a more thorough
description of the component types which play an active role in the testbench is given.

% driver
A driver has a port on which it can receive transactions. Normally, a driver enters an infinite loop in the \ttt{run}
phase where it wait for transactions and applies them either directly to the DUT or does so via a BFM. In order to
interact with the DUT, it is necessary in SystemVerilog to use a \textit{virtual interface} which effectively is a
handle to the pins of the DUT. The driver gets the next transaction through a call of \ttt{get\_next\_item} and
signals that the transaction has been applied by calling \ttt{item\_done}. The driver can provide feedback to the
code generating the sequence by passing a new transaction to the \ttt{item\_done} call. Special care has to be taken
when an interface is pipelined, i.e. when the DUT can accept multiple transactions before the first one is
acknowledged. In this case, the driver has to keep track of the transactions which are in flight and has to make sure
that the transactions are applied in the correct order \cite[ch. 4.7]{mehta2018asic}.

% monitor
A monitor holds, like the driver, a handle to the pins of the DUT. It observes the state of the pins and translates
the observed activity into transactions which are published via one or multiple analysis ports \cite[Ch. 4.3]{mehta2018asic}.

% subscriber
A series of subscribers can then receive these transactions and act upon them, for instance collecting coverage
information or comparing the transactions to a model. A coverage collector looks at the incoming transactions and
uses SystemVerilog functional coverage features to keep track of statistics such as whether a read was performed
after a write \cite[Ch. 4.3]{mehta2018asic}.

% scoreboard
The task of verifying that the transactions produced by the DUT are correct is handled by the scoreboard in the UVM.
The UVM does not prescribe a way of how exactly to achieve this. Since correctness has to be checked for an arbitrary
stream of transactions, some form of a model is usually needed. Depending on the complexity, this model could be
included in the scoreboard or be its own component which just like the DUT receives one or multiple transaction
streams and produces another set of transaction streams. The scoreboard is in this case reduced to comparing the
result streams of the DUT and the model \cite[Ch. 4.3]{mehta2018asic}.

\subsubsection{Configurability} %----------------------------------------------------------------------------------------------
%- Configurability
%  - factory
%  - configDB

The introduction of the component system, each with their well defined responsibilities, is only one part of the way
towards reusable and scalable testbenches. Not only concepts and abstractions are necessary, but also guidelines and
programming patterns for how the testbench code should be implemented. How can a specialized component be derived
from another for a specific test case? How can different versions of a component be easily swapped out? How can a
component implementation be made configurable in a way that also works well with derived components?

The answer to the first question is inheritance, which is available through SystemVerilogs OOP features. A component
can be derived from another and override methods to specialize its behavior. The answer to the question of how to
swap out different versions easily which the designers of UVM and its predecessors chose, is the factory pattern. In
a factory pattern, objects are not created through their constructor, but through a factory method which dynamically
determines which object to create, possibly based on a set of standardized parameters. In UVM, all UVM objects are
registered with a global factory. When a certain component A should be used instead of another component B, a factory
override can be issued, resulting in the creation of component A in all places where component B would normally have
been created. Of course A should be a subtype of B, such that A and B share the same interface. This allows for very
simples swapping out of components in a testbench without making the test code parameterizable with respect to the
different component implementations \cite[Ch. 13]{salemi2013uvm}.

Due to the global nature of the UVM factory, a one-siz-fits-all approach has to be adopted for what parameters may be
passed to the factory method. In the UVM, only the name for the object is given when an instance is produced:
\ttt{my\_class::type\_id::create("MyName")}. This raises the question of how to configure instances of UVM objects,
since it is impossible to pass parameters as it would usually be done in a constructor. The UVM solves this problem
with a configuration database, the so-called \ttt{uvm\_config\_db}, a global dictionary which is aware of the
component hierarchy in the testbench. One component can set a parameter in the configuration database and a
sub-component will be able to query the database for the parameter. In order to allow for a component to have
multiple sub-components of the same type with different configurations, the configuration database accepts a
hierarchical path when depositing a value in the dictionary. The deposited value will only be visible to the
referenced component and all its potential sub-components \cite{configdb}.

Together, the factory and configuration database provide the means for a test case to tailor the static testbench
environment to its needs. The factory is responsible for swapping out full implementations, while the configuration
database makes it possible to change the parameters a specific component works with \cite[Ch. 4.3]{mehta2018asic}.

\begin{comment}

- components are one step towards a reusable and scalable verification environment, providing concepts and abstractions
- but reuse and scalability also have to be tackled at the programming level

- not alone good enough to provide primitives
- we also need to concern ourselves with software design patterns to create resuable testbenches
- how do we swap out different implementations? -> factory
- how do we specialize an implementation? -> inheritance
- how do we make an implementation configurable? -> configdb

\end{comment}

\subsubsection{UVM Tests} %----------------------------------------------------------------------------------------------------

Now that the static infrastructure of a UVM testbench and the mechanisms for creating stimulus have been introduced,
all that is left for a complete testbench is the test case itself. The test case holds the actual purpose of why this
test is needed encoded in the stimulus stream it applies to the DUT. To achieve this, it creates an instance of the
top-level environment which will hold all the static testbench infrastructure. It may or may not configure the
environment such that it fits its needs. Then, the test case creates instances of relevant sequences and directs them
to the relevant sequencers in the environment. The checking of the DUT's behavior and other analysis tasks like
functional coverage collection are all handled by the static infrastructure of the testbench. The test case only has
to start the sequences and wait for the test to finish \cite[Ch. 4.3]{mehta2018asic}.

\subsubsection{Register Abstraction Layer} %-----------------------------------------------------------------------------------

Many designs today are rich in registers controlling the configuration and therefore the functionality of a DUT.
These configuration registers are usually attached to a bus and memory-mapped. A common task in setting up a test
case would be to set these registers to a specific state, which, with the tools presented up to now, would require
the use of sequences which target specific addresses through a bus interface. This can become a tedious task and a
abstraction of the interaction with the registers of a design would be useful.

The UVM Register Abstraction Layer (RAL) provides a standard way to abstract the registers of a DUT, independent of
the bus interface which is used to accesses them. A model of the registers inside the DUT is created using predefined
classes for fields, registers, blocks of registers but also memories mapped into an address space. These models can
be generated by commercial tools based on a description of the register layout \cite{uvm_ral}.

The accesses to the register model are either translated into actual transactions which are applied through the bus
interface causing side effects in the process, or they are performed through simulator specific backdoor accesses
which do not cause side effects in the DUT. The UVM also allows for configuring the model to a desired state before
committing it to the DUT in one go \cite{uvm_ral}.

The classes used to construct a register model capture detailed information about the registers, such as for instance
their accesses policies like read-only registers, but also more complex ones like writing a 1 to a bit to clear it.
This information is used to predict the result of a read or write to a register in the model, such that it stays
synchronized with the actual registers in the DUT without the need to use backdoor accesses in every register
accesses \cite{uvm_ral}.

\begin{comment}
  \cite{uvm_ral}
- many designs today are rich in registers controlling the configuration of the DUT attached to a bus
- a common task in a test case is to set these registers to a known state to test a certain configuration
- with the tools preseted up to now, we would have to use sequences with specific addresses and data to set the registers
- this is a very tedious task, and engineers would probably develope their own abstraction containing addresses of regs
- the UVM RAL provides a standard way to abstract the registers of a DUT, independent of bus interface
- a model of the registers in the DUT is created using predefined classes for fields, registers, blocks or memories
- accesses to the model are either translated into transactions on the bus or are performed through backdoor accesses
- thus model is connected to the actual registers in the DUT
- frontdoor access use transactions for access and cause sideeffects
- backdoor access uses simulator specific access methods and does not cause sideeffects
- also allows configuring the model with .set before commiting to DUT using .update

- register fields can have different access policies such as RO, RW but also special ones like write 1 to clear a bit
-

- models are usually generated from a register description language

-RAL is actually just a transactor system with a reg model to generate meaningful transactions
\end{comment}

\subsubsection{Utilities} %----------------------------------------------------------------------------------------------------

In addition to the core concepts and their implementation, the UVM also provides small utilities which are a part of
handling testbenches. This includes most notably a standardized reporting and verbosity system. Macros are provided
for infos, warnings, errors and fatal errors, which automatically annotate the message with the hierarchical path of
the component that issued it. At the end of running a UVM testbench, a summary of how many warnings and errors were
encountered is
provided to the user. The amount of information printed to the console can be controlled by setting a verbosity level
\cite[Ch. 19]{salemi2013uvm}.

Finally, the UVM provides many macros which simplify the writing of the test code and reduce the amount of
boilerplate code. For instance, each component should call the \ttt{`uvm\_component\_utils(name)} macro, which among
others takes care of the registration of the component with the UVM factory.

%\cite[Ch. 19]{salemi2013uvm}

\subsection{Open-Source Alternatives} %----------------------------------------------------------------------------------------

While the UVM standard itself is open and an open-source reference implementation exists in SystemVerilog, no
open-source simulator has implemented the SystemVerilog standard to a degree sufficient to compile the UVM reference
implementation \cite{Sasselli2023}. In this section, developments in the open-source verification community which aim
to bring modern verification features to new languages and environments are discussed.

\subsubsection{Cocotb} %-------------------------------------------------------------------------------------------------------

Cocotb is an open-source project under the FOSSi foundation, which focuses on providing a productive and
vendor-agnostic testbench framework for Verilog, VHDL and even mixed-signal designs \cite{cocotb}. Cocotb, short for
Coroutine-based Co-simulation Testbench, uses coroutines in python as an efficient way of having many simulation
threads cooperatively interface one simulation. Simulation events are modelled as asynchronous function calls which
can be "awaited", resulting in the waiting thread to suspend execution until the simulation backend triggers the event.

While taking advantage of the low learning-curve and rich library ecosystem of Python, Cocotb also suffers from the
drawbacks of python like the lack of static typing and the overhead of the Python interpreter. Due to its ease of use
and support for all major simulators, including commercial ones, Cocotb is a popular choice for a modern open-source
testbench framework. It does however not provide the infrastructure for building scalable and reusable testbenches
like the UVM does.

Due to its popularity, a series of other extensions have been developed around Cocotb. Examples are the
cocotb-coverage \cite{crvpython} and PyVSC \cite{pyvsc} libraries which both add functional coverage collection and
constrained random stimulus generation capabilities to a Cocotb testbench.

\subsubsection{Chiseltest} %---------------------------------------------------------------------------------------------------

Chiseltest was developed as part of the Chisel hardware construction language \cite{chiselpaper} and is a Scala-based testbench
framework \cite{chiseltest}. Due to a change in the Chisel compiler infrastructure, it has been recently deprecated,
but a replacement is under development by the Chisel community. While meant to be used with Chisel designs, Verilog
designs can also be included via Chisel. The number of supported simulators is more limited compared to Cocotb,
supporting only Verilator, Icarus Verilog and Synopsys VCS when using Verilog designs. Chiseltest is integrated with
the scalatest testing framework, which allows for an efficient way to organize and run tests.

Chiseltest provides a simple peek/poke/step interface to the simulation and allows for multi-threaded testbenches
using a fork-join model. However, it does not provide any
level of abstraction and infrastructure for building complex
testbenches, focussing on directed unit-test-style testbenches.

Work has been done on extending Chiseltest with formal verification features \cite{laeufer2021open}. This feature is
deeply integrated with the Chisel hardware construction language and allows the user to write formal properties in
the source code of the design in Chisel just like in SystemVerilog.

\todo{we need to talk about svsim}

\todo{also spinal HDL sim}

\subsubsection{ChiselVerify} %-------------------------------------------------------------------------------------------------

ChiselVerify is a verification library for Chisel designs built on top of Chiseltest \cite{chiselverify}. It aims to
improve the verification capabilities in the Chisel ecosystem by providing verification tools such as functional
coverage, constrained randomization and formal verification features. While showcasing how a BFM can be built using
scala and Chiseltest through a general AXI4 BFM, the framework does not provide guidelines and abstractions on how to
build standardized, scalable and reusable testbenches.

Functional coverage can only be collected at the IO of the DUT in ChiselVerify. It is not possible to collect
coverage at the transaction level which would require the monitoring of scala variables in addition to Chisel IO
ports. However, ChiselVerify allows for the expression of more complex cross coverage relationships. It not only
supports simultaneous hits between the cross-coverage bins but also timed relationships, for instance that a hit to a
bin results eventually in a hit to the crossed bin.

\begin{comment}

- functional coverage only on ports
- no sampling events for coverage
- but more complex timing relationship for cross coverage, i.e. not just simultaneous hits

- concerns itself with adding systemverilog verification capabilities
- does not provide methods to build reusable scalable testbenches

\end{comment}

\subsubsection{PyUVM} %--------------------------------------------------------------------------------------------------------

PyUVM is a project which aims to make UVM more accessible through the use of Python and by relying on open-source
tools \cite{pyuvm}. The project implements a subset of the UVM standard, which they describe as most frequently used,
in Python. As a motivation for implementing the project in Python, its ease of use due to light syntax and dynamic
typing are highlighted, and the existing verification infrastructure surrounding cocotb. The project tries to take
advantage of Pythons flexibility where possible to reduce the verbosity inherent in SystemVerilog. Among the UVM
features implemented in PyUVM are the factory, the configuration database, all component types, the TLM interface,
UVM phases, sequences and the register abstraction layer.

\subsubsection{UVM support in Verilator} %-------------------------------------------------------------------------------------

In addition to bringing modern verification features to new languages and environments to increase the accessibility,
work is also in progress to bring SystemVerilog UVM support to Verilator \cite{uvm_verilator}. Verilator is a popular
open-source
Verilog simulator which transpiles Verilog code to executable C++ models of the hardware design. The effort lead by
the Tools Workgroup in CHIPS Alliance aims to make Verilator fully capable of running UVM testbenches. In its current
state Verilator only supports a subset of SystemVerilog features. One crucial element is the addition of dynamic
scheduling in Verilator which is necessary for dynamically triggered events used in the UVM. The effort of enabling
all SystemVerilog features required for a full UVM testbench is still in an early phase.

Verilator does however allow for the integration with SystemC, a C++ library for system modelling. An official UVM
SystemC library exists, which can be used in combination with Verilator to create UVM testbenches in an open-source
environment \cite{Sasselli2023}.

\begin{comment}

\section{Software Design Patterns} %===========================================================================================

\todo{maybe this whole section can be dropped. We can introduce the patterns we use in the analysis section}

\todo{meta text}

Software design patterns offer general solutions to reoccurring problems in software design. They provide
template-like solutions which prescribe a certain structure and organization of code. They represent a variety of
best practice approaches to software design. Design patterns can be grouped into three categories. Creational
patterns are concerned with making the instantiation of classes more flexible. Structural patterns are concerned with
the composition of classes and objects. Lastly, behavioral patterns are concerned with how objects communicate and
interact \cite[Ch. 1]{design_patterns}.

One creational design pattern was already encountered when discussing the UVM: the factory pattern. The factory
pattern decouples the decision of which object to create from the actual creation of the object. In place of the
constructor, a function is used which returns an object with a known interface. The function can be provided in two
ways. It could be provided by a so-called abstract factory, which is an object with an interface known to provide the
factory function. It could also be provided by a factory method, which derived classes implement to define which
object should be created \cite[Ch. 3]{design_patterns}.

A class C can be written purely relying on the factory for creating objects with interface L. The end-user has the
option to define what object with interface L should be created everywhere \textit{without} modifying the code for
class C. The factory does not have to be a global singleton as in the UVM, but could also be a parameter to the
constructor of class C. Furthermore, a factory is usually specific to a certain group of classes which share an
interface and common parameters. This is in contrast to the UVM, where one factory is used for all components, with
the only shared parameter being the parent component and the name of the component \cite[Ch. 3]{design_patterns}.

There are other creational design patterns which also facilitate the swapping out of different implementations of
interfaces used in a class. The dependency injection pattern relies on the user to actually construct the object it
depends upon and then provide them to the class through its constructor or setter-methods. The class works with the
object handel it has been provided, without knowing which concrete implementation of the interface it actually works
with. Unlike the factory pattern, no infrastructure for object creation is created, but instead the responsibility of
object creation is pushed up to the user of the class \cite{ioc_di}.

Another useful pattern is the observer pattern. It is a behavioral pattern, which is also known as the
publish-subscribe pattern. In a system where multiple objects, the observers, are interested in changes of the state
of another object, the subject, this pattern describes how observers should be notified. It is the subject which
maintains a list of observers and has the responsibility to notify them when its state changes \cite[Ch.
5]{design_patterns}. This pattern can be seen in the UVM where analysis components like the scoreboard or coverage
collectors are notified of transactions on the DUT interfaces through the monitor. In this case it is not directly a
change of state which the observers are notified about, but the occurrence of an event.

- factory pattern
- inversion of control and dependency injection \cite{ioc_di}
- service locator pattern
  - kind of like factory, an object know how to create the right other objects, if global singleton it its kind of like factory
- strategy pattern
- observer pattern

- publish subscribe pattern
- actor pattern \cite{actors}

\end{comment}


\chapter{Problem Formulation and Method} %////////////////////////////////////////////////////////////////////////////////////

The UVM provides abstractions to compartmentalize a testbench in a standardized way. Instead of each engineer or
company defining their own abstractions, the UVM provides shared abstractions and a standardized application
programming interface (API). This makes it possible to quickly
understand an unknown testbench and to reuse parts of it in other projects. It also facilitates the exchange of
verification components between different entities and eases the hiring of new verification engineers.

However, the UVM is a complex and large framework. A lot of the literature on UVM focusses on how the UVM wants us to
build testbenches, but not necessarily why it was designed that way. Considering the process of its creation, it can
be seen that the UVM is something which grew
out of series of other methodologies and carries a lot of legacy with it. It seems the UVM always tries to anticipate
every possible use-case, leading to its size and complexity. It is therefore worth an investigation whether all of
its features are actually used in the industry and whether some of the features could be simplified.

While other open-source projects have explored bringing modern verification features like functional coverage,
constrained randomization and complex assertions to new languages, little concern has been given to how scalable and
flexible verification environments could and should be developed in the respective languages. Languages like Python
or Scala used in PyUVM or ChiselVerify bring more features to the table than SystemVerilog in terms of
general-purpose programming, but it is not clear how these can be taken advantage of to build scalable and reusable
testbenches. Could a productivity gain be achieved by adapting the concepts and abstractions of the UVM and their
implementation such that they better fit these high-level languages? Which parts of the UVM should be carried over to
such a revised and adapted methodology?

As such, it seems like there is a research opportunity to investigate the UVM, its design choices and its usage in
the industry in order to define a verification framework in a modern general-purpose language. There are two levels
to this process: on the one hand, the concepts and abstractions of the UVM have to be considered, and on the other
hand their implementation in SystemVerilog has to be examined. These considerations lead to the following research questions:

\begin{enumerate}
  \item Is everything in the UVM standard necessary and used by companies in the industry?
  \item Can the concepts and abstractions of the UVM and their implementation be condensed and simplified?
  \item Can the features of a modern general-purpose language be taken advantage of to build a more powerful and
    flexible verification framework?
\end{enumerate}

To answer these questions, the following methods will be used. First, interviews concerning UVM and verification
will be conducted through informal interviews with a small number of companies in the Copenhagen area which use UVM
in their verification efforts. The goal of the interview is to get an understanding of how the UVM is received in the
industry, how and what parts of the UVM are used and what improvements the companies themselves could imagine. Taking
account the insight gained from the interviews, the UVM will be analyzed to identify the design decisions behind it
and alternative design choices will be reflected upon to synthesize a set of requirements for a verification
framework embedded in a modern general-purpose language. Finally, an implementation attempt will be made following
the established requirements to evaluate the feasibility of the proposed framework and its ease of use compared to the UVM.

\begin{comment}

- a lot of the literature talks about HOW the UVM does things, but not necessarily WHY
- I want to try to answer that

- UVM provides abstractions to compartmentalize a testbench
- the engineer would else have to make them up himself -> each engineer would invent their own methodology
- UVM provides a standard way of doing things
- makes it easy to understand an unknown testbench
-> communication is also a big factor of why a methodology should be adapted

- can concepts in UVM be condensed/simplified?
- is all of UVM used/necessary for modern verification?
- is there a productivity gain in embedding a HVL as a DSL in a modern general-purpose language?
- what featues should such a model HVL DSL have?

- UVM seems to attempt to anticipate all possible use cases, but does not achieve this
- shouldn't a framework facilitate the most common use cases and offer a way of extending it for the less common ones?

method:
- informal interview with relevant companies in the industry from the copenhagen area
- look at other modern verification frameworks
- other published sources critically analyzing UVM

\end{comment}

% talk about company interviews
\chapter{Industry Interviews}
%////////////////////////////////////////////////////////////////////////////////////////////////////

\todo{meta text}

\section{Interview Outline} %==================================================================================================

The interviews were conducted relatively freely but with a set of questions in mind to guide the conversation. The
interviews started off with a set of questions about the company itself. This concerned facts like the size of the
verification department or the usual size and application of projects. The conversation would then move on to their
verification pipeline, starting from the specification to the tape-out. The tools used in the process would be
discussed alongside the verification methods employed. Of special interest was the fact whether the company had or
could consider open-source verification framework alternative like for instance cocotb. One area of interest was
whether any industry standards for testing like ISO standards were directly impacting the verification work. The
different levels at which a system were tested and their usage of reference models would be discussed.

From here, the interview would move on to UVM-specific questions. It would be discussed which features were used, how
well the reuse mechanisms were working and whether external verification IP (VIP) was used. Features like UVM
phasing, configurability mechanisms like the configuration database and the UVM factory would be discussed. Room for
criticism of the UVM would be given, and the interviewee would be asked to point out any limitations they had encountered.

The interview would then move to a general critical perspective, considering things like bottlenecks in the
verification process, unique issues the engineers had encountered and what they wished to see in future developments
in the verification area.

\begin{comment}
# General Questions

Verification Pipeline
- describe the general flow of your verification process from spec to signoff
- what are the interactions with the design team?
- do you use UVM

Automation
- have you integrated any automated tools or scripts into your verification flow to speed up repetitive tasks?

Standards
- Are you following any standards (e.g. ISO) in your verification process? If so, how do you ensure compliance?

Verification IP reuse
- how do you reuse VIP going from specification models to rtl to netlists
- do you develope your VIP inhouse or do you also obtain external VIP
- UVM is about reuse, how successful have you been in reusing components across different projects? And what difficulties have you encountered?

Special DSP needs
- have you done any special customization on top of UVM for DSP verification?

Verification methods
- how much do you use formal methods
- how do you define which parts to test using formal methods and which parts using simulation based methods

Tools
- which eda tools are you using
- have you looked at alternative frameworks like cocotb in python?
- cosim vs. compiled in SystemVerilog
- what is your opinion on integrating verification language with rtl language?

Test scales
- do you use different approaches for testing smaller design units

Coverage
- what coverage metrics do you use? Code coverage?
- what are the most important coverage metrics for you?

Debugging & verification
- how do you use your verification environment for debugging?
- do you use interactive running of testbenches?

Golden models
- how do you consrruct scoreboard?
- what do you put in your scoreboard?
- is it always a cycle/transaction accurate model?

Bottlenecks
- what are the biggest bottlenecks in your verification process? How do you address them?

# UVM Specific Questions

Phasing
- which phases do you use apart from uvm standard phases
- have you added custom logic to different phases

Configurability
- how satisfied are you with the configuration mechanisms in UVM
- how much do you use the configDB and uvm factory
- what would be the advantages over using formal parameters and generic class instead

Reuse
- how extensively do you use more complex inheritance to allow for reuse

# Open criticism

Criticism
- what limitations of uvm have you encountered
- Are there any language limitations that you encounter while developing your verification environment? If so, how do you work around them?
- did you have any use case where you needed to adapt your verification methods?
\end{comment}

\section{Company 1} %==========================================================================================================

Company 1 develops ASICs for the hearing aid industry, with a team of 7-8 dedicated verification engineers. The
verification tasks they are performing do not need to adhere to ISO standards directly, but these standards are
already captured in the verification plan. The company prefers using a single language for design to enable
incremental compilation and simplify the workflow. All verification IP (VIP) is developed in-house and actively
maintained, with reuse being a key focus across projects. Examples of reusable VIP include agents for standard interfaces like
APB and SWD. Additionally, they integrate models for external IP, such as EEPROM, using C or Verilog models. Reuse is
made easy and encouraged by maintaining a single code base for everything.

Once the specification for a new project is complete, verification begins in parallel with the design process. The
basic layout of the testbenches can be created early on, based on the interfaces being used. The company uses
module-level testbenches before moving to verification of the top-level. They make extensive use of different UVM
runtime phases, particularly reset phases, to improve modularity and composition. Over time, they have developed
their own standard scoreboard implementation and rely on the configDB to pass data between agents. Connectivity
checks are performed to ensure that specific configurations correctly link different parts of the design.

Their coverage strategy prioritizes functional coverage and FSM code coverage. Debugging is supported by
assertion-based verification, with assertions used for runtime checking, formal checking, or both. Debugging remains
an unpredictable part of the development process. Known bugs are particularly difficult to work around while trying to progress.

For simulation, the company uses VCS and employs continuous integration (CI) to manage regression testing. Each
regression run includes approximately 6000 simulations, with a total runtime of around six hours. Synthesis is also
performed as part of the regression flow, and coverage checks are automatically executed. The CI system ensures that
a freshly checked-out version of the project always works. Although they have considered PyUVM as a potential tool,
they find the speed insufficient for their needs. Furthermore, they are cautious about adopting new or niche tools,
fearing that it could complicate future recruitment efforts.

The testbenches of the company rely heavily on CRV. However, signal data streams are not purely random, but instead
they use typical data encountered in real-world scenarios, because change throughout time is difficult to capture in
constraints. To support DSP verification, MATLAB models are integrated into the verification environment.

The company is overall satisfied with their usage of UVM, but pointed out some shortcomings. Specifically, they
believe that code using the UVM factory is difficult to debug and maintain, while the register abstraction layer
(RAL) in UVM is incomplete. For instance, a single register block cannot be mapped into multiple address spaces, so
they developed their own extension to address this limitation. Concerning the UVM factory, they note that using
callbacks, a feature not available during the development of UVM, would be the better option to keep dependencies
modular in the code. They also observe that many UVM examples available online are outdated and focus only on trivial
cases. Additionally, they note that UVM was influenced by several companies of which some insisted on incorporating
features from their own verification methodologies, sometimes resulting in unnecessary complexity. While they
continue to use UVM, they are not opposed to a framework with fewer options and simpler design choices. They
also emphasize that power-aware verification remains a significant challenge.

In terms of recruitment, the company intentionally avoids using all UVM and SystemVerilog features to make hiring
easier. They believe that limiting the use of overly complex or non-standard features lowers the learning curve for
new engineers. They have observed that it is rare for software engineers to transition into verification roles, so
they do not see a need to tailor the verification environment specifically for software engineers. To further ease
the process of setting up a verification environment, they use a setup framework that can automatically generate the
basic structure for testbenches and unit tests.

One of the key challenges they face is that the test plan often becomes a bottleneck, more so than the actual
development time for testbenches. While development time tends to be predictable, debugging remains highly
unpredictable, especially when working around known bugs while trying to progress the verification effort.

\begin{comment}
Company 1 develops ASICs for the hearing aid industry.
- 7-8 dedicated verification engineers

- do not use all UVM and SystemVerilog features
- try to stick to standard features to make hiring easier
- develop all VIP inhouse
- reuse inhouse VIP across projects, e.g. interfaces like APB or SWD
- integrate C or verilog models for bought IP like EEPROM
- use VCS for simulation
- prefer single language for design to enable incremental compilation
- think that register abstraction in UVM is very complex
- have testbenches at the module level
- use randomized inputs (CRV)
- integrate matlab models for DSP
- Signal data streams are not random but represent typical data
- constraints in time, i.e. between signal packages, are difficult
- use FSM code coverage
- no line or branch coverage
- functional coverage

- after specification is finished, verification starts in parallel with design
- basic testbench layout can be created based on interfaces

- for debugging, assertion-based verification is used, some formal, some runtime, some both
- use connectivity checks to check that certain config connects certain parts of design

- for regression CI is used
- checkout always works
- 6000 simulations with tests
- 6 hours runtime
- synthesis is also done for regression
- coverage checks are performed

- no ISO standards have to be considered by the verification team
- standards are required for chip set, already captured in verification plan

- inhouse VIP is actively improved and maintained and reused
- 1 code base for everything

- have looked at PyUVM but speed is an issue
- want a setup framework to generate basic structure of testbench
- want a setup framework to generate basic unit test structure
- afraid of adapting new niche tools since it may be a recruitment issue

- in their experience it is rare that software engineers become verification engineers, do not see need to tailor verification environment to software engineers

- prefer formal methods over unit tests

- in terms of bottlenecks, the testplan is more of an issue than the actual development time of the testbenches
- Development time is also more predicatable, debugging is not
- especially working around known bugs to further progress is difficult

- use the different UVM runtime phases extensively, especially reset phases, to increase composition
- UVM factory is difficult to debug, and hard to maintain
- callbacks would be the bettern option but were not available when UVM was drafted

- have their own standard scoreboard implementation

- UVM is not perfect, some companies insisted on things from their own methodology

- use configDB to pass data to agents

- RAL is not finished in their opinion
- e.g. one reg block can not be mapped into multiple address spaces
- developed their own extension

- UVM examples on the internet are often outdated
- only showcase small and trivial examples

- power aware verification is still difficult

- like idea of a framework without too many choices
\end{comment}

\section{Company 2} %==========================================================================================================

Company 2 offers consulting services for hardware design and verification, including training for UVM, thus bringing
extensive experience in handling various verification challenges.

Verification at its core is about building confidence in the design in their opinion. The process typically follows
an evolutionary path: initially, few bugs are found, then many bugs surface as the verification infrastructure
matures, and finally, the design stabilizes with no remaining bugs. This approach often involves dividing the
verification task into smaller, manageable features and verifying them separately. Effective verification requires
robust coverage models, as weak models give misleading results. These coverage models can be validated through
intentionally weak tests, which are expected to score low. However, it is important to note that coverage only
accounts for known-knowns and known-unknowns; there is no established methodology for addressing unknown-unknowns,
i.e. bugs that are not detected by the verification plan. The company follows the best practice of always capturing
design assumptions in assertions.

Formal verification is considered a superior tool for proving design correctness by the company but often struggles with the
complexity of large designs. Despite these challenges, formal methods remain a critical component of their verification process.

They perceive UVM to be a well-working tool for verification. Like all tools, it is not perfect though, and they note
some small issues which they have encountered. The ConfigDB mechanism, can confuse users, in their experience, due to its
hierarchical scoping mechanism. Misunderstanding its usage can lead to spaghetti code, but when used correctly, the
ConfigDB solves the essential problem of enabling communication between classes without direct references. It acts as
a middleman, allowing information to be passed down from the test to the environment, agents, and drivers. Sometimes
they even use the ConfigDB as a channel, where a driver monitors specific keys for changes to receive data.

They note that the UVM factory mechanism generally works well for flexible component creation. In object-oriented
programming with static hierarchies, the creation of many small specific factories for component substitution in the
hierarchy would be necessary anyways. UVM's global factory addresses this need, reducing the programming overhead for
the end-user. But the UVM factory also introduces overhead, due to the indirection of object creation. Best practices
include using direct instantiation when no overrides are expected or limiting factory use to scenarios where dynamic
changes are really necessary.

Phasing in UVM works well for the company. But there are some issues concerning the communication of phase
completion. Each component can stop the phase from completing by raising an objection. However, this can lead to
problems when the main test code finishes injecting stimuli while the testbench still has to wait for responses. The
UVM predecessor VMM had a consensus object that allowed components to register their done conditions, offering a potential
solution. In agents, reset should be handled within the run phase. Base tests should handle reset and shutdown
phases, while derived tests should handle configuration and main phases. The company has developed its own standard
scoreboard infrastructure, which focuses on comparing streams of transactions.

They note that the Register Abstraction Layer (RAL) poses several challenges. It assumes a direct mapping of one
register transaction to one bus sequence item. This assumption can be problematic when protocols require multiple
transactions to complete the changes implied by the register transaction. Additionally, while the RAL supports
writing individual fields, certain protocols do not support this and would need a read-modify-write cycle, resulting
again in multiple sequence items. Locking registers by calling \ttt{.lock()} is available but does not ensure
immutability, raising questions
about its utility. Register randomization can be achieved using the `rand` keyword on fields, but unlike read/write
operations, calls to `rand` are not synchronized if multiple components access the register model in parallel. There
are currently multiple ways of injecting constraints into the register model, but none of them work well or are
verbose according to the company.

Certain types of DUTs present unique challenges. In a scenario where a DUT processes a stream, the interface where
the input stream is consumed may be separated from the one where the output stream is produced. This means the
resulting processed stream items flow though a different agent than the input stream items. An issue arises, if the
test case would like to make a decision based on the processed stream. The test case can only receive feedback from a
driver that it has sent a transaction to. In this case, it would like to receive feedback from a completely different
agent. This case is not considered in UVM. A solution was outlined, where the agent receiving the processed stream
would receive dummy transactions from the test case, such that it could answer with items from the processed stream.

DUTs that only produce outputs, such as random number generators, pose another issue: determining when to stop the
test. Timing-related challenges also arise when one has to determined whether transactions occur simultaneously
across multiple interfaces. The analysis ports of all interfaces would have to send either data or null each clock
cycle, reducing significant overhead, if this was to be determined. Another issue revolves around DUTs like filters,
which require initial stabilization before producing valid output. This could be handled by using metadata in
transactions or RTL signals to indicate readiness.

The company is satisfied with commercial verification tools, but they note that access to these tools remains a
challenge in educational settings, where they are typically available only during courses. They believe that this
educational space should be filled by open-source alternatives such as PyUVM, which they are actively exploring.

\begin{comment}

Company 2 offers consulting services for hardware design and verification, including training for UVM.

- one issue with the RAL is register randomization, they have their own implementation
- scoreboard: have their own standard scoreboard implementation
- model is not in scoreboard, scoreboard only compares streams of transactions

- first step is usually systemC model which can also be used for firmware development
- verification is about model checking
- assertions are alos a model
- verification process is about building confidence, first no bugs, then a lot, then no bugs again
- as verification infrastructure improves, the number of bugs found increases

- UVM falls apart in multi-clock designs

- assertions should be close to the interfaces

- a VIP should already contain a coverage model
- for system level coverage, cross coverage between different agents can be used

- coverage model has to be good, otherwise it is not useful
- can be check with intentionally weak tests

- design assumptions should be captured in assertions

- assertions have to be back-annotated to specification

- verification engineer also has implicit assumptions about how to attack the problem of proving that something is correct

- coverage only contains the known-knowns and known-unknowns
- no methodology to find unknown-unknowns

- for companies, the commercial tools are fine
- they see problem for education, where access to the commercial tools is only available during the course

- verification is about confidence in the design
- divide and conquer, by splitting into features and verifying them separately

- configDB often confuses people due to the scoping mechanism
- can lead to spaghetti code, if one does not know what they are doing
- but it solves one essential problem: talking to a class you do not know
- allows communication between two classes without them holding a handle to each other
- configDB is the middle man
- maybe it would be a good idea to have a configDB without scoping, too
- config is passed down from test -> env -> agents -> driver
- sometimes use configDB as channel -> driver monitors configDB for changes at a certain key

- satisfied with UVM factory
- in OOP program a static hierarchy exists
- if you want to change out components in the hierarchy, you would build your own micro-factory anyways
- why not have a global real factory
- factory comes with overhead though, so use new when it is known that no override will occur
- or limit construction to those times strictly necessary, e.g. at an actual value change

- phasing is a good idea
- reset and shutdown should be provided by base test
- config and main phase should be provided by the derived tests
- agents or any other component should only have a run phase
- one issue is the communication of when a phase is done
- raise and drop objection is used
- but sometimes the test code is done injection stimulus, but the TB should wait until all responses have been received
- VMM had a consensus object where objects register a "done" condition

- in an agent, the reset should also be handled in the run phase

- the scoreboard is application specific, but the infrastructure can be generalized
- this was presented in a paper at DVCon

- one issue is reactive slave
- DUT with interface to producer and consumer, consumer agent should be passive
- but what if test case needs access to values received by consumer?
- need to make consumer active and send one seq item per value, to get feedback
- can not use objections here, else you get stuck

- another issue are DUTs which only have outputs, like an random number generator
- how to know when to stop the test?

- another issue is timing related: when do transactions actually happen at the same time?
- for instance a coverage collector listening to 3 interfaces and should only sample if transaction at the same time on all three interfaces
- in current UVM, monitors would have to send null or transactions every cycle

- another issue are DUTs like filters where some data is needed for the DUT to stabilize and produce valid output
- could use meta data in the transaction
- could also use RTL signal as event to signal that the DUT is ready

- RAL is very big, 25\% of UVM class description
- can write fields individually, what if protocol does not support that? Would need read-modify-write cycle on the bus
- assumes mapping of one register transaction to one bus sequence item, what if multiple are needed?
- should allow translation of register transaction to sequence of bus transactions
- registers have a lock method, but that does not mean they are immutable, why is this there?

- randomization of registers can be done by using rand keyword on fields
- calls to rand are not synchronized though, in contrast to read and writes to the register model
- difficult to inject constraints into register model, without being verbose
- also difficult to randomize only one specific reg -> DVCon paper

- general perspective
- verification progress is not linear -> sometimes hard to communciate between engineers and managers
- the spec is not always perfect
- sees hope in AI DSL for spec
- formal is the better tool, but struggles with complexity

\end{comment}

\section{Company 3} %==========================================================================================================

Company 3 develops ASICs for the hearing aid industry. Their verification team consists of two dedicated engineers,
though verification responsibilities are shared with design engineers. Design engineers handle simpler verification
tasks, while the verification engineers focus on more complex aspects. The company follows a waterfall development model.

About six years ago, they transitioned to UVM and have since adopted a standardized subset of UVM to ensure
consistency across projects. Agents are frequently reused since most projects rely on the same interfaces. Their
Testbenches are employed not only for functional verification but also for gate-level simulations. While external
VIPs have been used and integrated successfully, most VIPs are developed in-house. Formal methods are used mostly for
their to application-specific instruction-set processors (ASIPs), but outside of that, formal verification is used
only in cases where it is easily applicable.

Their verification framework includes both module-level and top-level testbenches, with UVM environments from the
former being reused in the latter. They also maintain a basic test framework for simpler verification needs. The
employed reference model are mostly transaction-accurate and Matlab models are used for verifying DSP blocks.

The company uses code coverage, but mainly focusses on functional coverage. Like in Company 1, CRV is used except for
the modelling of audio streams where representative data streams are used instead. They note that debugging is one of
the most time-consuming parts of the verification process, often taking up to 70\% of the overall verification effort.

The team makes use of all UVM run phases, setting up the DUT and handling all analysis during the appropriate reset,
configuration and shutdown phases to simplify the work of test case developers. They find the UVM Config DB effective
for parameter passing, in part because they view parameterization of classes in SystemVerilog as cumbersome. Over the
years, they have developed their own UVM extensions, including a custom scoreboard implementation. One issue they
note concerning phasing is the handling of reset during the test. A solution is to do phase-hopping to go back to the
reset phase, which is a messy solution according to them.

Rather than seeing limitations inherent in the UVM, they see more limitations in SystemVerilog itself. For instance
parameterized interfaces can cause issues, and the language's verbosity and boilerplate code could be improved
according to them. Clocking blocks are also avoided due to frequent problems with multiple driver conflicts. While
they generally have no issues with the UVM factory, they believe it would be helpful if unsuccessful overrides
specified via the command line generated explicit error messages. The UVM factory is primarily used by the team to
adjust constraints and replace general sequences with more specific ones when necessary.

Overall, the team places a high priority on maintaining clear and understandable verification code, particularly so
that RTL engineers can easily work with it, too. They prefer straightforward, maintainable code over overly complex
features such as event callbacks.

\begin{comment}

Company 3 develops ASICs for the hearing aid industry.

- 2 dedicated verification engineers

- work in waterfall model
- verification engineer only involved in complex verification tasks, else design engineer does verification

- ISO standard requirements are already encoded in verification plan

- switched to UVM around 6 years ago

- many interfaces are repeated, agents can be reused

- reuse testbenches also for gate level simulation

- have used external VIP, was easy to integrate
- but many develope their own VIP

- there is a "standard" UVM subset

- CRV not used for data audio streams
- coverage is important though

- use code coverage, but disable it for some parts of the design

- use formal methods especially for their ASIPs
- else use formal only in obvious cases

- have their own basic test framework
- module level + top level testbenches
- UVM environment from module level testbenches is reused in top level testbenches

- reference models are cycle or transaction accurate
- some matlab models generate RTL -> model comes for free

- bottleneck is verification, around 70/%
- but out of this, debugging takes the longest time
- soemtimes need to argue why error is actually not an error
- communication overhead and manpower

- use all UVM run phases
- set up everything behind the scenes for the test developer

- config DB works well for them for passing parameters
- thinks that parameterization of classes does not work well in SystemVerilog

- clocking blocks offen suffer from multiple driver problems, because the tools can not understand them
- tend to not use clocking blocks

- does not see a problem with the UVM factory
- only unsuccesful overrides through CLI do not give errors, but should
- are used to change constraints
- used to replace general sequences with specific ones

- have own UVM extensions and methodology
- have own scoreboard implementation

- one issue is reset handling in CRV
- own agent for reset handling
- phase hopping is difficult and messy

- see most issues with SystemVerilog limitations
- parameterized interfaces
- verbose
- boiler plate

- but prefers guidelines over limitations for UVM

- prefers to keep verification code simle and understandable, also for RTL engineers
- does not like overly complex features like event callbacks

\end{comment}

\section{Takeaways from the Interviews} %=================================================================================

\todo{revise}

In general, companies seem to be satisfied with what the UVM can offer them. They acknowledge that it may have some
shortcomings, but it is an industry standard which most importantly is supported by all major EDA vendors, saving the
companies from the infamous vendor-lock. Having a standard also eases the hiring process, since new employees knowing
UVM can more easily understand the existing infrastructure.

It is also clear that only a subset of all the functionality which UVM offers is actually used by the companies. This
subset seems to consist of the core ideas behind UVM and seems to be generally accepted. Where UVM does not satisfy
their needs, companies have gone to implement their own extensions like for the scoreboard of the register
abstraction layer. This of course partially defeats the purpose of UVM, but is also a product of the universalism of
UVM, where abstractions have to fit all use cases and thus may need extra work before being able to be used for any
of those use cases.

Although formal methods are acknowledged as superior, allowing quick and exhaustive checking of a certain feature,
all companies only use it where the performance of current tools allows them to. Dynamic verification is thus still
highly relevant and the reusability mechanisms in the UVM seem to be appreciated and used by the companies to lower
the effort of developing new testbenches.

Some companies note languages limitations in SystemVerilog itself which proves the point of investigating
verification frameworks in more powerful languages. Due to long simulation times for their tests, performance is of
concern though, and has inhibited the use of co-simulation frameworks like cocotb in the production environment. Also
compilation times seem to be an issue for some companies.

As it can be seen in the example provided by Company 2, UVM is not perfect in facilitating dynamic tests, where the
test case produces stimuli as a reaction to the DUT's responses when these are spread over different interfaces. The
model of how the test case can get feedback from the DUT should therefore be investigated.

The opinions on the UVM factory differ, with some companies seeing it as a key part of creating flexible testbenches
and others seeing it as the source of hard to debug errors. It could be interesting to investigate what other options
exist and how user friendly they are.

The RAL seems to be a common point of criticism, with all companies pointing out that it has some issues. This should
also be investigated.

\begin{comment}
- companies are generally satisfied with UVM
- they see it as a tool which may have some shortcomings but it is a standard which is supported by all 3 major EDA vendors
- they only use a subset which seems to be generally accepted across the industry
- where UVM does not satisfy their needs, or does not specify a specific way of handling something (like scoreboard),
they have developed their own extensions

- compilation times seem to be an issue for some companies

- the approach of having IP units per interface seems to work well, and encourages reuse

- the phasing system seems to be good to facilitate composability of test scenarios

- although formal methods are acknowlegded as superior, where they work, they can not be applied to verify all parts of
a design and a simulation based framework is still needed

- There are different opinions on the UVM factory, it could be interesting to investigate what other options exists
and how user friendly they are

- all agreed that the RAL has some issues
- especially the mapping of one register transaction to one bus sequence item is problematic and easily addressed

- some point out that SystemVerilog itself limits what UVM can do, so it could be interesting to investigate what
other languages could do in the context of hardware verification
- performance is of concern though

- there seem to be some issues related to how the test case code gets feedback from the testbench in order to write
interactive test case code
\end{comment}

\chapter{Analysis and Design} %////////////////////////////////////////////////////////////////////////////////////////////////

In this chapter, a design for a verification framework embedded in a modern programming language will be developed.
The solutions that existing tools and frameworks offer will be analyzed to uncover general concepts and separate them
from the specific implementations. These concepts will be used to develop requirements for the new framework and a
solution which is tailored to the host language will be proposed.

\begin{comment}
- here, the requirements should be made and possible approaches outlined
- this includes possible tools to rely on (also for features which are not part of the final product, e.g. coverage and crv)

- there are two levels to the design:
- the concepts, abstractions that the testbench is strucured by
- the coding principles which facilitate the flexible nature of the testbench

story:
- first define the scope of this project
  - we look at UVM features as the current status quo and evaluate them and if they are considered useful, determine how to be integrate them into a new framework
- then some basic decisions have to be made since they limit the design choices
  - the simulation backend has to be chosen
  - the verification language has to be chosen
    - should it be a embedded DSL or a dedicated Language
    - reflection should be made even though the answer is clear -> DSL is the only realistic path
    - here the approach using CIRCT should be mentioned
- having established the basic infrastructure, the requirements can be made
- first the basic simulation infrastructure has to be established
  - how should the simulation be interacted with?
    - what kind of data types are necessary to interface with the simulation?
    - how should the interface be captured in the DSL?
    - what actions can be performed on the interface?
    - what are the timing semantics? clocks and steps
    - timing resolution?
    - concurrency -> the simulation thread
    - thread interaction with simulation
    - interthread interaction
    - test entry point
- then the test bench abstractions and concepts should be defined
  - what phases does a test bench have?
  - what are the basic components and the basic structure of a test bench?
  - how are test cases defined?
- then design patterns for reuse should be defined
  - how can components be swapped out?
  - how are components configured?
\end{comment}

\section{Scope} %==============================================================================================================

The aim of this project is to provide a capable verification framework which takes inspiration from existing frameworks such as Cocotb, ChiselTest and ChiselVerify to provide a useful simulation interface and which takes inspiration from UVM to extend and co-design this simulation interface with a library of primitives for building reusable testbench infrastructure. This is quite a daunting task, involving many different aspects from simulators, to API design, to concurrent programming and verification methodology. In order to make this task achievable within the time frame of this project, the scope has to be narrowed down.

In order to be able to build a library for reusable testbench infrastructure, some basic simulation functionality is strictly necessary. An interface to the simulation of an RTL design has to be provided. This interface has to allow for the control of the simulation by multiple threads in a safe manner such that multiple components like they are found in UVM can run concurrently. This multi-threaded simulation interface should allow for the creation of simple unit tests as they are found in ChiselTest and serve as the basis for complex structured testbenches as they are found in UVM.

While features like constrained-random stimulus generation and function coverage are essential to a modern verification framework, they are not part of the scope of this project. These features have already been explored in the context of Python and Scala and could be integrated into the new framework at a later point. 

Concerning the library for reusable testbench infrastructure, the focus will be on providing UVM-like capabilities while also trying to simplify the design choices and reduce the complexity of the framework. This means that not all features of UVM will be integrated into the new framework, and not all features will exhibit the same complexity as they do in UVM. This also necessarily means that the new framework will not be as \textit{universal} as the UVM itself claims to be. The goal is instead to identify a solid subset of UVM concepts and features which provides primitives to build standardized and composable testbenches. These features should cover the three main aspects of a testbench as they have been identified by \citeauthor{sutherland2015uvm} \cite{sutherland2015uvm}: the static testbench infrastructure in the form of components, the stimulus aspect of the testbench in the form of sequences and finally the composition of both in the form of test cases. Penetrating all these features is the phasing system, which should therefore also be part of the scope of this project.

While these named aspects lay the foundation of a composable testbench, other aspects like the register abstraction layer (RAL) are not strictly required, but make a certain aspect of the testbench construction easier by introducing additional abstraction. Although the company interviews have shown that the RAL is a point of criticism in UVM which should be investigated further, it had to be removed from the scope of this project due to time constraints.







\begin{comment}

Before a design can be proposed, the overall scope for the verification framework has to be outlined. The goal is to develop a
verification framework which is embedded in a modern programming language and which takes inspiration from UVM for
reusable testbenches. The framework should provide a set of features such that it could be used instead of
SystemVerilog and UVM for the development of testbenches.

At its core, the framework has to provide a simulation interface in order to allow the user to interact with an
instance of the DUT. The simulation interface should allow for multiple threads to control and access the DUT
concurrently. The tool responsible for simulating design should support general RTL designs.

A modern testbench would not be complete without a way to generate constrained randomize and collect coverage
information. As such, mechanisms for both features in the language of choice should be provided. \todo{but since this
  has been explored in python and scala already, not of concern for me, but a small consideration of how it may be
integrated will be made}

The previous requirements provide the foundation for constructing testbenches. To facilitate the development of
reusable testbench infrastructure, the framework should provide a set of primitives similar to those found in the
UVM. This includes some form of verification base components responsible for different tasks in the testbench, a
phasing system to structure the testbench execution and a way to organize stimulus into sequences. All these features
should be easily composable into testbenches and test cases.

\todo{in UVM sim interface and lib are separated, here we have the chance to co design them -> therefore own
simulation interface from scratch}

\todo{also put a focus on co-designing the simulation interface and the library}

\todo{should functional coverage and crv be considered outside of the scope from the start? But why talk about them
in the background then. Maybe they can be described more shallow}

\todo{while allowing for the development of complex testbenches ala UVM, the framework should also aim to simplify}

\todo{what parts of UVM should be integrated?}
- here \cite{sutherland2015uvm} can give an answer
- it describes a practical subset of UVM which allows to construct UVM testbenches
- it acknowledges that UVM has a lot of redundant features and features which are only useful in rare cases
- it recommends a subset of UVM as a quick way to exploit the benefits of UVM
- this shall be the basis for the integration of UVM features into the new framework

\todo{elaborate}

- a library of primitives for reusable testbench infrastructure

- purpose: ideas for a verification framework in a modern language which takes inspiration from UVM for reusable testbench components and could provide a replacement for SystemVerilog+UVM
- not all features will be implemented necessarily
- on the one hand, a simulation framework for hardware designs
- should be multi threaded
- the level of simulation depends on the backend (for instance 4 state logic)
- could be its own language,but this would be too complex for the time frame
- instead an embedded dsl should be used
- a modern language should be used

- simulation framework should feature coverage collection integrated with simulation backend for sampling
- should feature constrained randomization library for random objects

- on the other hand, primitives for a reusable testbench environment should be provided in a library
- there are two levels to this design process:
  - the abstractions and concepts
  - the coding principles which facilitate the flexible nature of the testbench
- it is clear that the coding principles are deeply interwoven with the implementation
- this includes some form of verification components responsible for different tasks in the testbench
- some kind of phasing system should be designed to compartmentalize the execution of the testbench
- some form of organization of stimulus into sequences should be provided
- a easy interface for running tests should be provided
\end{comment}

\section{Verification Language} %-------------------------------------------------------------------------------------------

The first choice to be made is, which language the verification framework should be integrated into. On the one hand,
this can be seen as an implementation choice, but on the other hand, the chosen language may limit the design choices
in terms of capabilities and syntax. Therefore the decision is made prior to designing the functionality of the
framework. The examples
which have been discussed were cocotb and PyUVM using Python, but also Scala being used by Chiseltest and
ChiselVerify. Before reflecting over these languages and other options, it is also worth considering the advantages
and disadvantages of embedding a verification framework into another language compared to developing a standalone
language such as Vera or even integrating it into the design language itself such as it is the case in SystemVerilog.

One of the primary advantages of embedding a verification framework into a general-purpose language instead of
developing a standalone hardware verification language or a shared design and verification language, is the effort
associated with developing all the necessary tools and infrastructure. A standalone language requires its own
compiler or interpreter which will have to be maintained. Any additional functionality which is not part of the
language itself has to be developed from the ground since no library ecosystem exists, as it is the case for
general-purpose languages. Additionally, many modern languages provide a dependency management system, which can be
leveraged to share and reuse code. Finally, quality of life features such as IDE support of the host language can
also be leveraged.

When performance is considered, a combined design and verification language has a clear advantage, since everything
can be compiled and optimized together. In a standalone language or an embedded language, a simulation has to be
interfaced by a runtime. Depending on the implementation, this can lead to a significant performance overhead.

When it comes to ease of use and the learning experience, dedicated languages have the advantage of being able to
tailor their syntax to the domain, making it easy and concise to express the concepts of the domain. On the other
hand, a whole new syntax has to be learned, which can be a hurdle for new users. For an embedded language, if the
host language is already known, the learning curve is lower than for a standalone language since it is equivalent to
learning a new library. If the host language is not known, the learning curve is much higher though, since aside from
the verification constructs a whole language has to be learned.

Due to the time constraints of this thesis, and the focus on higher level infrastructure for testbenches, embedding a
verification language into a general-purpose programming language is the only realistic choice. When it comes to the
choice of a host language, several factors have to be considered. The language should have object-oriented features
to allow for inheritance based composability. It would be interesting to also have functional programming features
such as closures and higher-order functions to be able to experiment with different design patterns. While the
language should be performant, it should not be a low-level language with explicit memory management, since this
would introduce the chances of memory bugs. Instead a garbage collected language would be preferable.

Python is an interpreted language with dynamic typing, object-oriented features and closures. Java is a compiled
object-oriented language with garbage collection running on a virtual machine. A modern alternative to Java is Scala,
which offers object-oriented and functional programming features and runs on the same infrastructure as Java. Many
other languages either do not offer all required features, are too low level or are tailored to a specific domain,
making them less attractive as a host language. While Java is a proven language, Scala offers more modern features
and a more concise syntax, making it the preferred choice. This leaves Python and Scala as the two main candidates.
Python has the advantage of being easier to learn and having a larger ecosystem of libraries, while Scala generally
offers better performance. One significant difference between the two is that Python is dynamically typed, while
Scala is statically typed. The dynamic typing of Python can make it easy to write code, but it also means that
basically all errors are only caught at runtime. For potentially very large testbench systems with long running
simulations, this should be considered a drawback. Considering this alongside Scala's flexibility in implementing
embedded languages with a natural syntax, it is decided to implement the verification framework in Scala.

Scala has received a major update with the release of Scala 3. This new version includes an overhaul of the syntax to
make it more lightweight. It also re-envisioned many of the more advanced features of the language such as contextual
abstractions, which allow the passing of implicit parameters to functions, as well as metaprogramming features. The
new version also includes a more powerful type system, which allows for more expressive types and more type-level
programming. While Scala 3 is clearly the more future-proof choice and offers many improvements over Scala 2, many of
the existing work in the Scala ecosystem such as Chisel and Chiselverify is incompatible with Scala 3. Since the
purpose of this project is to explore the integration of a verification framework into a modern programming language,
this language should be as expressive and feature rich as possible in order to explore all possibilities. Especially
the improved type system in Scala 3 which offers new ways to compose and extend types is of interest for the
development of a verification framework with a focus on reusability. Therefore, Scala 3 is chosen as the host
language for the verification framework, in spite of the incompatibility with existing Scala 2 projects.

\begin{comment}
  DSL pros:
- reuse of host language features
  - libraries -> especially true for python
  - expressiveness in higher-level languages
- a DSL requires less infrastructure
  - no compiler/interpreter has to be developed and maintained
  - leverage IDE support of host language -> here true software languages have a big advantage over HDLs
  - dependency management may come for free
- easier to learn since it is a subset of the host language
- sometimes hard to integrate DSL into the look and feel of the host language while still providing easy syntax

DSL cons:
- performance may be worse
  - in traditional HDL setup, verification code and design code are compiled directly together
  - in DSL setup, it depends on the implementation
    - message parsin interface with simulator?
    - linked library?
    - JVM to native transitions?
- may also be hearder to learn since the whole language has to be known to take full advantage of the DSL
  - a standalone language only has required features for verification

HVL pros:
-

- also a question of multi-language environments
  - should design and verification be done in the same language?
  - the advantage of a common language is that some verification features are better placed in the design code like assertions
  -

- especially to experiment with a new features a DSL is the only realistic choice since the overhead of developing a new language is too high

- OOP
- functional would be nice to try out what it can do (expands options for design)
- garbage collected
- performant
- static typing

\end{comment}

\section{Simulation Backend} %----------------------------------------------------------------------------------------------

Another key decision which has to be made, concerns the simulation backend which the verification framework will use.
Like the choice of host language, this has implications on the design of the framework, such as whether 4-state logic
or only 2-state logic is supported.
The decision is influenced by the choice of supported HDL's, ease of interfacing with the simulation and
finally of course performance. Since the project itself aims to be open-source, a proprietary simulator
is not an option.

The two most popular open-source simulators are Icarus Verilog and Verilator. Icarus Verilog
compiles a Verilog design into its own intermediate representation which can be executed by a separate simulation
runtime engine, a kind of interpreter \cite{iverilog}. Using Icarus Verilog as a simulation backend would require
interfacing with this simulation runtime. It is built to support nearly all of the IEEE-1364-2005 Verilog standard,
but also supports some of the SystemVerilog features of the IEEE1800-2012 standard.

Verilator, on the other hand, transpiles the Verilog design into C++ or SystemC classes which can then be compiled
to machine code \cite{verilator}. This makes Verilator very suitable for co-simulation with other models
or software components, since it can be easily interfaced at the C/C++ level. The only interface to a verilated
model, apart from setting inputs and reading outputs, is the \texttt{eval} function which runs a static schedule to
update the model's state. The model is used simply by creating an instance of the associated C++ class, assigning the
input variables values and evaluating the model. Since the model is in the end compiled to a native executable with
no additional simulation
runtime, the performance of Verilator is significantly better than Icarus Verilog, with speedups of 100x on a single
thread being reported by the developers \cite{verilator}. Verilator supports nearly fully the IEEE 1364-2005
standard, partially the IEEE 1800-2005 standard and some very specific features of newer standards. The IEEE
1800-2005 standard includes SystemVerilog features, of which some like classes and interfaces are supported by Verilator.

Before version 5, Verilator ignored delay statements in the Verilog source code which are necessary to add timing
information to a simulation. The newer version now support this feature, thus matching the capabilities of Icarus Verilog.

One feature not supported by Verilator is the use of 4-state logic. Verilator only supports 2-state logic and uses
static analysis of the source code instead to identify initialization issues and warn the user about them. Icarus
Verilog supports 4-state logic as defined in the Verilog standard.

An new alternative to Verilator is ESSENT, a high-performance Verilog simulator, which is capable of doubling the
performance of Verilator \cite{beamer2021essent}. It does however use FIRRTL, an intermediate representation for RTL
designs, as input. The Slang SystemVerilog compiler \cite{slang} is capable of translating SystemVerilog designs to
FIRRTL, but does not support all features of SystemVerilog yet. The additional translation step from SystemVerilog to
FIRRTL is a disadvantage compared to the other simulator options. This fact alongside the maturity of both Icarus
Verilog and Verilator, makes ESSENT not the prefered choice for the simulation backend.

In terms of language support and capabilities, both simlators are close to each other. The lacking support of 4-state
logic in Verilator is deemed acceptable, since initialization errors are still caught. Considering the ease of
integrating verilated models with other software components in addition to the
superior performance, Verilator is identified as the better option for a simulation backend of the verification framework.

%\section{Requirements} %=======================================================================================================

\section{Simulation Interface} %--------------------------------------------------------------------------------------------

A test writer can interact with a simulation in two ways. On the one hand, the ports of DUT can be accessed, i.e.
driving inputs or reading the state of a port, and on the other hand the advancement of time in the simulation can be
controlled. Concerning the former, a mechanism of referencing ports is required. Furthermore, each port has to be
associated with a type which decides what values a port is allowed to be driven with and how the bits sampled on a
port should be interpreted, e.g. as a signed or unsigned integer.

Controlling time in a simulation is about letting time progress until points of interest. These points of interest can
be absolute points in time, but also events like signal transitions. At the most simple level, a simulation interface
can allow for delays which for instance advance time by a certain amount of nanoseconds. This becomes quickly
tedious, since most of the time, one is interested in events such as clock edges which would have to be checked for
manually. A more powerful abstraction is to let the user wait for events like rising edges of signals as a
synchronization point. The intent of the test code becomes more clear this way. However, the timing of when inputs
should be driven and outputs should be sampled relative to the clock edges still has to be managed explicitly by the
user. This mechanism is what SystemVerilog and Cocotb provide.

In a sequential digital design, the system behavior is defined by the clock. Each clock cycle, new inputs can be
applied and lead to a state change once the active edge of the clock occurs. A new abstraction can thus be created,
where in each clock cycle the test code can sample outputs to decide new inputs and then advance the simulation to
the next clock cycle. This way, the test code is always executed in a time region where the clock is stable and the
user does not have to worry about aligning the simulation time to the clock edges. The Vera \cite[Sec. 7]{flake2020a}
HVL and the Chiseltest \cite{chiseltest} framework both use this approach. In Vera, the user controls time using the
\ttt{@N} syntax, where \ttt{N} is the number of clock cycles to advance time by. In Chiseltest, the
\ttt{clock.step(N)} method is used to advance time by \ttt{N} clock cycles. For purely sequential designs, this
interface allows for concise and readable test code.

It does however delegate the responsibility of driving the inputs and sampling the outputs at the correct time to the simulation
runtime. The question is, when are the correct times to sample and
drive? Chiseltest has a simple solution: The time resolution of a simulation is fixed to \SI{1}{ps} while the clock
is fixed to a \SI{2}{ps} period. This means that the simulation is always advanced from falling edge to falling edge
of the clock. In Vera on the other hand, the user can specify skews relative to clock edges for when inputs should be
driven and outputs should be sampled on a per signal basis. This allows for more accurate timing modeling, which may
for instance be necessary in gate-level simulations with delay models of the gates.

% The question is however, when is the test
% code actually run within a clock cycle? It should be run after all outputs have been sampled to have the newest
% version of their state, but also before the new inputs are driven. With inputs which have to be driven early and
% outputs which have to be sampled late, this is not possible. It is not clear which semantics Vera chooses for this
% case. Instead of sampling all outputs at once, the runtime could sample them only when accessed, advancing time only
% when necessary. This could still create situations where an input missed its drive slot in the current clock cycle.
% Considering these issues, it can be seen that the introduction of the clock cycle abstraction for advancing time has
% its own challenges. The solution Chiseltest provides, limits the modeling capabilities in terms of driving and
% sampling of inputs and outputs. This is acceptable for designs at the RTL where usually no component delay is
% modeled. Vera on the other hand, tries to model the interface timing accuratly, but this opens questions about the
% ordering driving and sample events and dependencies between them.

\begin{comment}
  {signal: [
  {name: 'clk_a', wave: 'p...p', period:2},
  {name: 'in_a', wave: '34.3.4.3.'},
  {name: 'out_a', wave: '5.6.5.6.5'},
  {name: ''},
  {name: 'clk_b', wave: 'p..',period:3},
  {name: 'in_b', wave: '78.7.8', period:1.5},
  {name: 'out_b', wave: '9.3.9.',period:1.5},
]}
f
\end{comment}

%multi-clock designs
Neither Vera nor Chiseltest support testing a design with multiple clock domains \cite[Sec.
7]{flake2020a}\cite{chiseltest_multiclock}. In Vera, the interface to the DUT is associated with a single clock. In
Chiseltest, the Chisel \ttt{Module} base type contains a single clock which is driven by Chiseltest. Due to the lack of
fine-grained control of time in either language, clock ports can't be driven with arbitrary frequencies by user code either. Cocotb gives the user direct access to time and allows therefore for the creation of arbitrary and multiple clocks. It is up to the user to synchronize himself with the clock edges, which gives maximum flexibilty but also requires more work from the user. 

For the verification framework, it is chosen to use a \ttt{step} interface to control time like in ChiselTest. It is
however extended to support multiple clock domains. It is decided to drive all signals at the falling edge of the
clock domain that they are associated with. An example for two clock domains is shown in Figure \ref{fig:wavedrom}. Inputs in a clock are changed at the falling edge of the clock by the user, while the sequential design reacts at the rising edge of the clock. Since inputs are actually driven \textit{after} all threads have finished their work, the affects of the change in input on mealy outputs, i.e. outputs which are a function of the input, are first visible in the \textit{next} clock cycle to the user. This is a limitation which simplifies the building of the simulation interface, but could be addressed in future extensions. The option of adding skews like in Vera, could also be added in the future. 
\begin{figure}
\centering
\includegraphics[width=\textwidth]{diagrams/wavedrom.pdf}
\caption{Waveform showing the timing of signal changes for inputs and outputs with respect to their clock domains \ttt{a} and \ttt{b}.}
\label{fig:wavedrom}
\end{figure}

In order to support multiple clocks, the explicit definition of periods has to be possible. This information will be captured in an interface definition which captures the inputs and outputs of the DUT in
Scala. While Cocotb can take advantage of dynamic typing to automatically add fields to the DUT object at runtime,
Scala requires us to declare a class for the DUT statically. This interface defintion should be declared as a class.
It has to point to the underlying source files and contains fields for each input and output of the DUT. The adopted
syntax for declaring an interface is shown in Listing \ref{lst:interface}. Clock ports and reset ports are inputs to
the design. Inputs and outputs are associated with a data type. The data types are inspired by Chisel
\cite{chiselpaper}. Finally, the inputs and outputs are associated with a clock and optionally a reset through the
\ttt{domain} function.

The API for interacting with the DUT is inspired by ChiselTest. Inputs provide \ttt{peek} and \ttt{poke} methods,
while outputs only provide a \ttt{peek} method. Clock ports have a \ttt{step(n)} method to advance the simulation by
\ttt{n} clock cycles. An additional method \ttt{stepUntil(predicate: => Boolean)} is added which accepts a closure evaluating to a boolean. Calling this method will advance the simulation until the clock cycle where the closure evaluates to true. 



Up until now, the DUT has been considered a black box which can only be interacted with through its ports. For some designs this can limit the observability severly. In Cocotb, the user can access any internal signal of the DUT. In the current ChiselTest version, access to internal signals is not possible. For the verification framework, is is chosen to allow limited acccess to internal signals. Only registers shall be accessable and it is only possible to read their value. In order to allow for the reading of internal registers, an additional function is added which can be used in an interface declaration to give access to a register. For instance, access to the state register could be given by declaring \ttt{val state = Reg(3.W, "state")} in the interface. The name provided in the string is the hierarchical path starting from inside the top module. To read the register, the \ttt{peekReg} method is used, which returns the integer value of the register.


%listing
\begin{listing}
\begin{lstlisting}[language=scala, captionpos=b, caption=Example for an interface declaration in Scala. Each data signal is assocaited with a clock domain.,label=lst:interface]
class MyInterface extends ModuleInterface(
          "path/to/file1.sv", "path/to/file2.sv") {
  val clk = ClockPort(5.ns)
  val rst = ResetPort()
  val data = Input(UInt(8.W))
  val valid = Input(Bool())
  val ready = Output(Bool())

  domain(clk, rst)(data, valid, ready)
}
\end{lstlisting}
\end{listing}

\begin{comment}

- two issues:
- how to control the interface of the DUT i.e. the pins
- how to manipulate time

- the first requires a way to reference ports, associate them with hardware types and set and get their values
- the second is about to progress to points of interest in simulation time
- question is what are the mechanisms taking you to that point in time
- easiest: just allow delays, i.e. wait for x ns
- possible to write all test code but it quickly becomes annoying to handle time
- instead the core points of interest can be seen as events which the test code can wait for
- rising edges of signals
- changes in signals
- falling edges of signals
- a certain passage of time can also be seen as event
- this allows for more readable and powerful testbenches
- can we abstract more?
- sequential design in hardware means clock cycles
- each cycle inputs may be changed, potentially based on outputs, then the clock ticks and outputs change
- for a simulation of clock based design it is thus enough to have a way to advance to the next clock cycle
- this is what Vera uses with its @1 syntax and chiseltest with its step command
- no need to worry about aligning to the clock edges, code will always be executed in a region where the clock is stable
- this is a powerful abstraction which allows for easy writing of testbenches

- but what about multi-clock designs?
- in pure event based testbenches, each clocks rising edge can be used as an alignement point
- in Vera the single clock associated to the interface is used
- in chiseltest .step could be called on multiple clocks but this will cause an error because it is not implemented

- of course this pushes the responsibility of driving signals at a specific point in time to the framework
- in chiseltest driving signals is always done at the falling edge of the clock assuming positive clocks
- thus the simulation is always advanced from falling edge to falling edge
- in vera, skews relative to the clocks are possible
- inputs are driven respecting the skew
- outputs are samples respecting the skew
- this allows for the modelling of more accurat timing for instance in case a gate level simulation with accurate
gate delays is used
- this opens the question: when is the test code in a clock cycle actually run
- in order to have the newest version of all outputs, the test code as to wait until the output which has to be
sampled latest in time
-

- we need to support basic data types to be able to describe the interface to the DUT
- what about complex data types like structs or interfaces?
- verilator sets limitation, no interface at top level
- structs are translated into wide bit vectors with fields concatenated
- what about arrays?

- like in Vera, the user will need to declare ports to the DUT

- what is the multiple driver behavior?
- in the case where one thread releases the valid signal and another wants to assert it?
- this requires different regions...
- release should only be applied if no other driver exists

- cocotb seems to allow for monitor region using "await ReadOnly"

- black-box or also white-box/grey-box? we could expose the internal state of the DUT to the testbench (i.e. register access)

\end{comment}

\section{Concurrency} %-----------------------------------------------------------------------------------------------------

Concurrency is a necessary feature for complex testbenches. It makes it easy to separate different tasks into
separated units which progress through time in their own manner, for instance waiting for different transitions to
occur. For hardware designs with multiple interfaces this becomes especially important, since each interface may be
controlled by its own thread instead of having to interleave the control of all interfaces in a single thread.

SystemVerilog and Chiseltest both use a fork-join model for concurrency. A code block can be executed in a new thread
by the keyword \ttt{fork}. The parent thread will continue its execution after spawning the new thread, but may wait
for the spawned thread to finish by calling \ttt{join}. A thread waiting for another will sleep until the clock cycle
when the child thread finishes. This is a simple model which allows for easy parallelization of tasks.

The same model shall be used for the verification framework due to its familiarity and ease of use. One question
which has to be answered is how the running of the threads is coordinated with the simulation. Here the following
semantics are chosen: Threads can be in two states, running or sleeping. While running, they can interact with the
DUT, for instance by driving signals or sampling outputs. Once they reach a \ttt{step} command in their code, they
will sleep and inform the simulation when they should be woken up again. The simulation will wait for all running
threads to reach a sleep point before advancing to the next clock cycle where a thread wants to be woken up.

Since all threads run concurrently, the ordering of \ttt{peek} and \ttt{poke} accesses is non-deterministic. The
accesses are serialized in the simulation runtime. Multiple \ttt{poke} accesses to the same signal will cause a
warning, but the last value will be used. Sometimes a thread has to monitor an input signal for a certain transition.
To be certain that all \ttt{poke} commands have occurred before the monitor thread receives the signal value, the
\ttt{peekMonitor} command is used. No more \ttt{poke} commands can be issued after this command before the next time
step, since this may invalidate the value another thread received when calling \ttt{peekMonitor}.

In the UVM, components communicate via TLM channels. TLM channels by themselves only facilitate method invocation by
one component on another. Only when a TLM FIFO is used, is communication between to concurrently running threads
facilitated. For the verification framework, a simplification is made such that only one channel primitive exists. This
channel allows for sending and receiving of messages between threads. The channel is not buffered, meaning that the
sender and receiver will be synchronized
on the exchange of a message. While waiting for the sender or receiver, depending on the operation, the thread will
be put to sleep until the clock cycle where the message is sent or received. This could be the same clock cycle it
went to sleep or a later one.

For the communication between components, it is however benificial to think about component ports and connecting
them, as it is done in the UVM. Like this, it is not necessary to directly pass a reference to a channel object to
the components which should be connected through the channel. Instead, both declare ports, one a receiver port and
one a sender port. The parent component can then connect the two ports.

The simulation starts in one root thread which can be forked into multiple threads. The simulation finishes when the
root thread exits. It can use the \ttt{join} functionality to wait for other threads to exit before finishing the simulation.

\begin{comment}
- concurrency is a key feature for working with hardware designs which are inherently parallel
- threads can be running or sleeping
- while running they can interact with the DUT
- when they have done all they need to do for current clock cycle, i.e. they call step, they sleep until their wakeup time
- all threads need to have reached a sleep point before the simulation can advance

- fork join is used in SystemVerilog and Chiseltest
- offers an easy way to spawn threads

- join puts thread into sleep mode until the clock cycle where the forked thread finishes

- communication between threads is important
- for this purposes channels shall be used
- channel objects allow to send and receive messages between threads
- when waiting for a message, the thread is put to sleep until the clock cycle where a message is sent
- channels are not buffered, sending will put to sleep until clock cycle where message receiver is available

- simulation starts in one root thread which can be forked
\end{comment}

\section{Test Phases} %-----------------------------------------------------------------------------------------------------

Test phases are a way to structure the execution of the testbench, essentially adding a set of synchronization points
to the testbench such that different components agree when a certain functionality should be executed. The UVM
defines an extensive set of phases to standardize when exactly which tasks should be handled. The purpose of the run
phases like \textit{reset}, \textit{configure} and \textit{main} is clear: They add certain checkpoints in the
simulation to synchronize components, and allow better inheritance based composition. A base test could for instance
define what should be done in the \textit{reset} phase, while specific test cases only have to implement the test
code in the \textit{main} phase.

In \cite{uvm_phases}, it is argued that having multiple build phases is necessary to ensure that components exist
before they are referenced. Since UVM components are classes and can be instantiated at any time in constrast to
modules which are static, it is argued to be necessary to add these phases for coordination and avoiding errors. The build phases are however not run in parallel. This means, the order in which components are created and connected is deterministic and known.

In this framework, a simpler approach is chosen which builds on the fact, that in the component hieararchy, a component
should only ever reference its child components and do so only after they have been created. Scala encourages the use of
the class constructor to fully initialize an object. The declaration of fields and the construction code are merged
inside the class body. Referencing fields before they have been initialized results in an error. The constructor of a
component should start by preparing the parameters for its child components and then instantiate them. After the
child components constructors have returned, they can be assumed to be fully initialized. It is thus possible to
connect channels of child components. All this
code will run in zero simulation time and instantiating the top-level component will result in a fully constructed
and connected testbench hierarchy. Due to Scala's way of handling constructor code and field definitions, it is
impossible to reference uninitialized components using this approach. When inheritance is used, Scala automatically
calls the super-class constructor before the one of the derived one, such that all fields of the super-class can be
assumed to be initialized in the constructor of the derived class.

The reason for having a \textit{end of elaboration} phase according to \cite[Ch 4.6]{mehta2018asic} is to finalize
the configuration of components. It is decided that all configuration of a component should happen in the constructor
of its parent, thus removing the need of the phase. A \textit{start of simulation} phase could be useful for printing
of configuration and test information according to \cite[4.6]{mehta2018asic}. Since this is not strictly
necessary, it is left out of the framework.

The run phase is called \textit{simulation} phase and the main phase is called the \textit{test} phase in the
verification framework. In the UVM, the sub-phases of the run phases each have their pre and post variant running
immediately before and
after the associated phase. According to \cite[Ch 4.6]{mehta2018asic}, not much use is anticipated for these phases
and they are left out, too.

This leaves the cleanup phases \textit{extract}, \textit{check}, \textit{report} and \textit{final} which are run \textit{after} the simulation has finished. It is necessary
to have at least one phase after the simulation has ended to give components a chance to analyze observed
transactions and coverage statistics in order to report results. Viewing phases as synchronization constructs, the
division into multiple phases is thus not strictly necesary. What it does facilitate is composability through
inheritance. A component could use the \textit{extract} implementation of its super-class and only specify new
behavior for the \textit{report} phase. However, this could also be achieved by the super-class declaring a
\textit{extract} method independent of the phasing, which the derived class can use in its \textit{report} implementation. How the extracting and
checking functionality is split into methods can be up to the user and does not have to be part of the phase system.
As such, it is decided to only include a \textit{report} phase which should be used by all analysis components to write a report to the console or a file. How the data for the report is prepared is up to the component.

Scala has support for traits which describe a set of methods and fields which a class has to implement. One class can
implement a series of traits. This makes it possible to make phases opt-in, by implementing the relevant trait in a
component. This makes it clear which phases a component uses. This can be useful when extending an existing
component, since it can be read from the class definition which phases are implemented.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{diagrams/own_phases.pdf}
  \caption{The simplified phasing system of the verification framework.}
  \label{fig:phases}
\end{figure}

The final phasing system is shown in Figure \ref{fig:phases}. The constructors are not an actual phase but are still
shown for clarity before the start of the run phases. The phase system is kept very simple, but it should, at least
for standard cases, provide the synchronization points necessary to structure the testbench execution.

One callenge a distributed testbench faces is the decision of when a phase has finished. In UVM, components can raise
an objection, which that they are not yet ready to proceed to the next phase. They signal that they are done by
dropping the objection. This way also a driver could for instance block the run phase from finishing until it has
received its last response. In the verification framework a less powerful but simple approach is chosen. Components
running code in the simulation phase have no way to object to the simulation finishing. Instead the reset, configure,
test and shutdown phases finish once all participants have returned from the method associated with the phase. The
simulation phase is thus meant for components which provide a static service in the testbench such as a driver or
monitor. The reset, configure, test and shutdown phases are meant for components which \textit{control} the testbench
in a fixed number of steps and exit once they are done. This way the simulation phase can be used to provide services
to the testbench which are always available, while the other phases are used to control the testbench execution.

An issue that the controlling phases have, is that some actions which they trigger may take an unpredictable time to
finish. In this case, a controlling phase such as the test phase should interrogate service providing components such
as drivers to determine when all its actions have taken effect. This could for instance be done by waiting for
sequence to finish producing stimulus, or by waiting for a monitor to observe a certain number of transactions on an interface.


\begin{comment}
- core concept in UVM
- the use case for the runtime phases is to modularize the test phases such that they can be composed and overwritten
through inheritance
- argument for the build phases is not so clear
- why does the build phase have to be different from the constructor of a class?
- why does the connect phase first have to come thereafter
- build phase is top down -> makes sense since components instantiate their children
- connect phase is bottom up
- \cite{uvm_phases} says it is necessary to ensure that components exist before they are referenced and since they
can be created at any time, it is necessary to ensure that phases are followed
- but constructor is also a function not a task in SystemVerilog -> no time passes

- in a components contructor:
- it sets the parameters for the children
- it calls the constructors for the children
- it now has references to fully constructed and connected children
- it connects the children and exposes the interfaces of children

- this way a component is fully constructed and connected when the constructor finishes
- it is thus impossible to have a not fully initialized component hieararchy
- and it can be seen that the top down and bottom up approach still holds:
- the instances are created top down because a component is always created by its parent
- but the connections are done after all children have been constructed -> the leaf component connects first
- connect is rippling up before each contructor exits

- whatever a component needs to do should only affect itself and its children
- after the children have been constructed, they can be used
- so any setup before the simulation starts should be handled in the constructors of the components
- all build phases are replaced by the constructors

- what about inheritance?
- the constructor of the base class should always be called first in the constructor of the derived class to ensure
that all fields are initialized

- what about the extract, check, report and final?
- extract and check serve the purpose of preparing report
- why not let it be up to the one reporting how to prepare the report?
- if the argument is that it is better to compose using inheritance (if extract does not need to be overwritten but
check does...), why not let the user decide what function should be inherited
- in terms of necesssary synchronization for the components, only a report phase is necessary to ensure that all
components have finished their work
- no real concrete use case is given for final
- after report, nothing should be output, no sim is running, what should be left to do?

- this leaves us with the run time phases
- run phase is necessary for components like drivers to always be available to receive transactions
- all finer granularity run time phases seem to have their use case
- the pre and post ones not really
- as \cite[Ch. 4.6]{mehta2018asic} notes, not much use is anticipated for the pre and post run time phases
\end{comment}

\section{Reusable Verification Primitives} %--------------------------------------------------------------------------------

Providing a set of primitives and a standard manner of composing them is a key part of facilitating the reuse of
verification components. The verification framework will follow the recommendations of \citeauthor{sutherland2015uvm}
\cite{sutherland2015uvm} for a minimal UVM subset required for practical verification according to the UVM. The
primitives which are mentioned by the authors match with those generally presented in the literature. These base primitives are:

\begin{itemize}
  \item The sequence item: The transaction primitive
  \item The sequence: Generates transaction sequences
  \item The driver: Drives transactions on the DUT's ports
  \item The sequencer: Arbitrates multiple sequences' access to the driver
  \item The monitor: Observes the DUT's ports and publishes observed transactions
  \item The analysis component: Subscribes to transactions and performs analysis or checks
  \item The agent: Contains all components necessary for a single interface
  \item The environment: Contains agents and other components necessary for a (sub)system
  \item The test: Contains the top environment and controls stimulus generation
\end{itemize}

Each of these components has a distinct role in the static verification environment. As such, each of them is needed to build a complete testbench, and the framework should
provide base classes for each of them which the user can extend.

In UVM these base components are implemented in a class hiearchy with \textit{uvm void} being the base class. Derived
from this is the \textit{uvm object} which provides a series of features like printing, recording, copying, comparing
as well as packing and unpacking, i.e. serialization, of objects. The \textit{uvm report object} is derived from
this, adding logging funnctionality. The \textit{uvm component} base class is derived from the \textit{uvm report
object} and adds phasing, hierarchy tracking and the factory interface. UVM sequence items and sequences are derived
from the \textit{uvm object} class.

As noted by \citeauthor{sutherland2015uvm} \cite{sutherland2015uvm}, not all of this class hiearchy is relevant to
the end-user of UVM. In the framework, three base classes are provided: Components, Transactions and Sequences.
Instead of relying on inheritance to share functionality among these as it is done in the UVM, traits shall be used
to compose the required functionality of these primitives.

A Component is the base type for all the static elements of the testbench. It is hierarchy-aware by holding
references to its parent and children. Phasing is not a direct part of the Component base type. Instead, as it has
been discussed previously, phases are added to a component by adding traits to it. For instance a component can
execute code in the simulation phase by extending the \ttt{SimulationPhase} trait and implementing the \ttt{sim}
method. Components add logging capabilities with hierarchical awareness which will be further discussed in Section
\ref{sec:logging}.

The Transaction base type is used to capture the stimulus items which are sent to the DUT. For transactions it is
important to be able to check for data equality. This is done by implementing the \ttt{equivalent} method.
Furthermore, it is important to copy transactions, which is facilitated by implementing the \ttt{copy} method.
Serialization and recording of transactions is considered outside of the scope of the framework. The Sequence base
type is used to generate transactions. It's design will be considered in more detail in Section \ref{sec:stimulus_sequences}.

While the channel type for the verification framework has already been introduced, for components a port system like it is found in the UVM offers more flexibility. Instead of having to pass references to channels to components when they are constructed, ports allow for the connection of channels after the components have been created. For this reason, a sender and a receiver port shall be provided which each provide a \ttt{connect} method to connect with the opposite port type. 

It was discussed in previously that the simplified system to decide when a phase should finish may require test cases to interrogate service providing components such as drivers or monitors to determine when they have finished their work. For this purpose, synchronization methods for waiting for a certain number of transactions shall be provided. A driver has the \ttt{waitForNumOfDrivenTxs(n)} method which blocks until the driver has driven \ttt{n} transactions in total. A monitor has the \ttt{waitForNumOfObservedTxs(n)} method which blocks until the monitor has observed \ttt{n} transactions in total.

\begin{comment}

- in uvm all extend uvm\_object and then uvm\_component
- uvm obj provides
  - factory interface
  - printing -> toString
  - recording -> outside of scope
  - copying -> should be defined for transaction type
  - comparing -> case classes support it, but for general classes user has to define, Ordered[T] and Comparable[T] exist in scala
  - serdes -> outside of scope

-

\cite{sutherland2015uvm} -> what is a practical subset for UVM

- \cite{sutherland2015uvm} will be used as reference for which features are necessary for a practical verification framework
-

- things that are needed
- a test primitive
- an environment container primitive
- an agent primitive
- a sequencer primitive
- a driver primitive
- an monitor primitive
- an analysis component primitive (subscriber)
-

- a UVM testbench is close to a hdl test harness with static components

- do we actually need static components? why is the driver not just a function? transactors are just functions
mapping one abstraction level to another...
- the sequencer is just a generator, a thing we can get new values from, a driver is just a function, it is the test
case that should call the driver and let that code block until the transaction is done, or fork the interaction to continue

- the whole control flow surrounding sequences in the UVM seems weird
- a sequence aka generator should be passed to a sequencer, which should run in its own thread and generate
transactions on the driver port pulling from different incomin sequences aka a channel mux in gears
\end{comment}

\section{Stimulus Sequences} %----------------------------------------------------------------------------------------------
\label{sec:stimulus_sequences}

Sequences are a necessary mechanism to structure the generation of stimulus such that it can be reused and composed.
The UVM design of sequences is quite straight forward and easy to use. All the user has to do is implement the
\ttt{body} method in which items can be generated. The communication with the receiver of the produced items is done
through the \ttt{start\_item} and \ttt{finish\_item} methods. The \ttt{start\_item} call blocks until the sequencer
grants the sequence the right to produce an item, while the \ttt{finish\_item} call actually sends the item and wait
for the response from the receiver.

In the verification framework, the syntax should orient itself more towards generators. In F\# for instance, a new
item in a sequence is produced calling \ttt{yield item}, while a sequence of items can be produced one after the
other using \ttt{yield! seq}. For the framework, a call to \ttt{yieldTx(tx)} shall produce the item, and return
the response item of a possibly different type. This means only a single function call is necessary to produce an
item and receive the response.
Other sequences can be added to the stream of the sequence using \ttt{yieldSeq(seq)} which forward the items of that
sequence and provide it with the received responses.

It is not always necessary to use a generator to produce stimulus. The advantage of the generator is that it can have
a complex stateful behavior which is captured the code of its body. If simple sequences, like for instance a
fixed-length sequence of random items, are needed, standard Scala collections should be used. For instance the
\ttt{Seq.fill(n)(fun)} function can be used to generate a sequence of \ttt{n} items by calling the \ttt{fun} function
\ttt{n} times. The \ttt{Seq.tabulate(n)(i => fun(i))} function can be used to generate a sequence of \ttt{n} items by
providing the function \ttt{fun} with the index of the item. In order to allow for the composing of Scala collections
and sequences, \ttt{yieldSeq} also should accept a Scala collection as an argument. Furthermore, a Scala collection
can be converted to a sequence using the \ttt{.toSequence} method. An example of a sequence is shown in Listing
\ref{lst:sequence}. Two items are produced using the \ttt{yieldTx} method, while two more items are produced using a
scala \ttt{Seq} collection.

Sequences can be composed by creating a new sequences which coordinates the execution of multiple sequences. In the
case of a simple composition, standard operators should be provided to compose sequences. Some ideas for sequence
combinators are:

\begin{itemize}
  \item \ttt{Sequence.concat(a,b,..)}: Concatenate sequences \ttt{a}, \ttt{b}, ...
  \item \ttt{Sequence.interleave(a,b)}: Interleave two sequences \ttt{a} and \ttt{b}
  \item \ttt{Sequence.mix(a,b)}: Randomly forward items from either sequence \ttt{a} or \ttt{b}
  \item \ttt{Sequence.repeat(a,n)}: Repeat sequence \ttt{a} \ttt{n} times
  \item \ttt{Sequence.map(a,f)}: Apply function \ttt{f} to each item of sequence \ttt{a}
  \item \ttt{Sequence.filter(a,p)}: Filter items of sequence \ttt{a} using predicate \ttt{p}
\end{itemize}

Some of these operators like \ttt{map} and \ttt{filter} take inspiration from common functional programming
higher-order functions.

\begin{listing}
\begin{lstlisting}[language=scala, captionpos=b, caption=Example of a sequence producing four items of type \ttt{Tx} with responses of type \ttt{Resp}.,label=lst:sequence]
class MySeq extends Sequence[Tx, Resp] {
  def body: Unit = {
    val r0 = yieldTx(new Tx(0))
    val r1 = yieldTx(new Tx(1))
    val s = Seq.tabulate(2)(i => new Tx(i + 2))
    val Seq(r2, r3) = yieldSeq(s)
  }
}
\end{lstlisting}
\end{listing}

To actually use a sequence, it has to be handed of to a sequencer. In the framework, a simple sequencer shall be
provided which plays one sequence after the other on a first-come-first-served basis. A sequence is played by calling
the method \ttt{sequencer.play(seq)}. The call blocks until the sequencer is ready to start palying the sequence.

Often, it is of interest for a test case to know when a sequence has finished, i.e. all items have been handed off to
the driver and the responses have been received. This can be done by calling the \ttt{seq.waitUntilDone()} method
which blocks until the sequence has finished.

\begin{comment}

- scala sequences should be convertable to sequences

\todo{what about virtual sequences}
- virtual sequences are about targetting different sequencers
- could yieldItem(seq, item) be used to target a different sequencer?

- sequences are a necessary mechanism to structure the generation of stimulus
- their implementation in UVM is quite good
- a sequence is a generator of transactions
- it is not clear why sequence needs two methods: start and finish item
- a sequence should prepare an item, then yield it
- the call to yield should return the response item
\end{comment}

\section{Running a Test} %-----------------------------------------------------------------------------------------------

In order to run a test case which the user has written, it has to be handed to a runtime which executes the different
phases. This is handled by the \ttt{runTest} function. It takes takes an instance of the DUT's interface and a
closure to create the test case. The closure receives the DUT instance as an argument. If case the testcase expects a
BFM instead of the direct interface to the DUT, the user can construct it in the closure before returning the test
instance. The \ttt{runTest} function retunrs once the test is finished. An example of how to run a test is shown in
Listing \ref{lst:runTest}.

\begin{listing}
\begin{lstlisting}[language=scala, captionpos=b, caption=Example code for running a test case using the verification framework.,label=lst:runTest]
runTest(new MyDUT) { dut =>
  new MyTest(dut)
}
\end{lstlisting}
\end{listing}

In addition to complex UVM-style testbenches, the framework should also support simpler tests. The Chiseltest
framework provides a simple way to write unit-test-like directed tests in a concise manner. These purely rely on the
peek, poke and expect functions or a BFM to interact with the DUT. This should also be possible in the verification
framework. The \ttt{Simulation} function is provided for this purpose. It takes the DUT instance and a closure which
contains the test code. An example is shown in Listing \ref{lst:unitTest}.

\begin{listing}
\begin{lstlisting}[language=scala, captionpos=b, caption=Example code for running a test case using the verification framework.,label=lst:unitTest]
Simulation(new MyDUT) { dut =>
  poke(dut.a, 1)
  poke(dut.b, 2)
  dut.clk.step(1)
  dut.c.except(3)
}
\end{lstlisting}
\end{listing}

\section{Testbench Configurability} %---------------------------------------------------------------------------------------

In the UVM, a factory for objects and component is used to keep the testbench flexible. The factory allows the
overwriting of one type with a derived one. This allows for instance for an easy exchange of one component implementation with
another, because the instantiation of objects is delegated to the factory. The interface for using the factory is
standardized. General UVM objects can pass a string to the factory to set the UVM name of the object, while components can pass a
name and a parent component reference. This leaves no option to pass parameters to the constructor of a component.
Instead, the UVM provides the configuration database, a key-value store, to share configuration parameters between
components. The configuration database is hierarchy-aware, meaning that each component has its own view on the
configuration database, and can thus receive a unqiue configuration. The configuration database also facilitates the
sharing of configuration parameters between components, since a key-value pair available at one level in the
hierarchy is also available at all child levels.

The factory is a powerful tool to make code flexible by design. A component has to be programmed once, but can be
configured in many ways by swapping out the implementation of its sub-components without chaging the code. But the
usage of the factory also makes the use of the configuration database necessary which circumvents any built in way of
passing configuration parameters to classes, such as through the constructor. Accessing the configuration database in UVM
requires manual error checking to ensure that the expected key-value pairs actually exist.

Another mechanism in the UVM to factilitate configurability is through callbacks. A callback encapsulates a set of
functions. A component can depend on these functions. The actual implementation of the callback can be chosen
dynamically, allowing for the behavior of the component itself to be changed. According to
\citeauthor{sutherland2015uvm} \cite{sutherland2015uvm}, this mechanism should however be avoided, due its convoluted
setup and performance overhead. \citeauthor{sutherland2015uvm} argues that inheritance meachnisms should be used instead.

In Scala, functions are first class citizens and can be passed around as objects. This makes it easy to pass
functions to components, which may depend on them for a specific task. The component only has to provide an interface to
exchange the function implementation, either through a constructor argument or a method, to allow for functionality similar to UVM callbacks.

The discussed mechanisms are not the only options to decouple a class implementation from its dependencies, i.e. other
objects it relies upon. Dependency injection is a design pattern which prescribes that the dependencies of a class
should be provided by its environment and not instantiated in the class itself. This can be done through the
constructor or through setter methods for the dependencies. By using interfaces for the dependencies to capture the
provided functionality, the class is maximally decoupled from the actual implementation of the dependencies.


Each of these three approaches have their advantages and disadvantages. While the factory facilitates the swapping of
components simply by calling the factory instead of a constructor, it creates an indirection in the passing of
parameters with components relying on string keys to locate their respective configuration parameters.
This can be error-prone and no help can be provided by the compiler to ensure that all necessary parameters are provided.

Dependency injection on the other hand allows for the usage of constructors or configuration methods to pass
parameters, which lets the compiler check their validity. On the other hand, it requires the code to be written in a
way which anticipates the swapping of components. A component always has to consider whether the dependencies of its
children should be hardcoded or instead kept flexible by exposing the dependencies through its own constructor or a method. This
would make top-level components extremely complex if they would have to collect all the dependencies of their
children either through their constructor or through a series of methods.

Using callbacks could be achieved more easily in Scala than in SystemVerilog. But the component has to anticipate the
functionality which should be exchangable just like in dependency injection. For smaller verification libraries, dependency injection could be a straightforward way to achieve configurability. For
larger libraries, the factory approach seems to be more suitable due to the fact that the configurability does not
pollute the code.

Using dependency injection does not need any infrastructure from the framework, since it only relies on language
primitives. As such it is always an option to use dependency injection if one wants to avoid the factory and
configuration database. The verification framework should however provide a factory mechanism and a way to pass parameters
akin to the configuration database which is tailored to the capabilities of the language.

In Scala, complex type relations can be expressed in the type arguments of a generic function. As such \ttt{[T <:
Component, R <: T]} can be used to express that \ttt{R} is a subtype of \ttt{T} which is a component. This can be
used to make a factory safe, such that only subtypes can override a type. Furthermore, Scala has an extensive metaprogramming subset which allows to inspect code at compile time. The factory shall rely on runtime reflection to create objects. Runtime reflection allows for the dynamic creation of objects using so-called class tags. Using this approach, no type registration with the factory is required. However, the constructor of the class must follow an expected signature. Trying to create an object of a class which does not have the expected constructor signature using runtime reflection would cause a runtime error. This is where Scala's metaprogramming capabilities come into play. A call to the factory for type \ttt{T} can check at compile time if the constructor of \ttt{T} has the expected signature. The same applies for overriding types with subtypes, where overrides are only accepted if both types follow the expected constructor signature. This makes the usage of the factory statically safe, with invalid usage being rejected at compile time. It also necessitates the usage of seperate factories for each primitive type, since the exptected constructor signatures may differ.

For the verification framework it is decided to provide multiple factories: Components, sequences and transactions each have their own factory, taking care of setup steps specific to each primitive and expecting a constructor interface specific to each primtive. Types can be overriden in a factory using the \ttt{overrideType[A,B]} method. The defining of key-value parameter pairs are done through the \ttt{Config} singleton.

The \ttt{Config} singleton provides a \ttt{set(k,v)} method to set a configuration parameter and a \ttt{tryGet[A](k)} method to attempt to retrieve a configuration parameter of type \ttt{A} with key \ttt{k}. The latter return a Scala \ttt{Option[A]} type which is \ttt{None} if the key does not exist or the value cannot be cast to type \ttt{A}. It returns \ttt{Some(v)} if the look-up was successful, where \ttt{v} is the actual value of type \ttt{A}. A method \ttt{get[A](k)} is also provided which throws an exception if the look-up fails. Furthermore, a \ttt{getOrElse[A](k, default)} method is provided which returns the default value of type \ttt{A} if the key does not exist or the value cannot be cast to type \ttt{A}. This is a more concise interface than the UVM configuration database exposes.

A demonstration of the API to the factories is shown in Listing \ref{lst:factory}. The factory objects \ttt{Comp}, \ttt{Sequence} (not \ttt{Seq} since that name is used in Scala) and \ttt{Tx} provide \ttt{create[A]} methods to produce an object of type \ttt{A} or the type which \ttt{A} is overridden with. The code from line 10 to 21 shows how a unique configuration can be created for a component. The factory \ttt{create} method accepts a key-value pair map itself which is visible to the object that is created. Instead of passing parameters directly to the factory, another mechanism is also provided which allows for the creation of a configuration context. Calling \ttt{Comp.builder} returns a builder object which can capture parameters and overrides using the \ttt{withParams} and \ttt{withOverride} methods. A builder could be bound to a variable and used to create multiple objects with the same configuration. 

When \ttt{set} is called inside a component, the created parameter is visible to itself and all its decendants. However, should its parent have defined the same key, the value of the parent has precedence. This way a parent can overrule default type overrides and parameters in all its decendants. 





\begin{comment}
  There are two ways a factory could be
implemented in Scala which have implications on whether a type first has to be registered before it can be used with
the factory or not. Since this affects the design of the API, the approach has to be decided as part of the design.
If a type has to be registered with the factory, a closure could be provided which the factory uses to construct new
instances. The closure is subject to Scala type constraints, making this method completely safe, i.e. a call to the
factory to create a given class will always work. Another approach could use runtime reflection to create new
instances of a class. In this dynamic approach, the constructor of the class \textit{must} have a standardized
signature. If this is not the case, for instance for a component with an integer argument, the factory would fail.
This approach is thus less safe, but does not require explicit registration of a type with the factory. Since the
passing of parameters using a mechanism akin to the configuration database does not come with any compile time checks
either, it is decided to use runtime reflection and thus no registration of types with the factory is necessary.


The creation of the different primitives in the framework such as components, transactions or sequences, have
different implications. The creation of a component involves the management of hierarchical references to make the
parent aware of its children and vice versa.

%%%%%%%%%%%% new ideas
- the static part of the testbench is a tree of components with potentially many configuration parameters and
swappable components
- therefore a factory really has an impact because all swappable components do not have to be communicated to the top level
- the config allows for easy parameter sharing and overriding throughout the hierarchy

- for sequences and transactions, this is not really the case
- they are created by the test case -> the test case knows what it wants to do
- but the test case may set up standard impl to drive seq -> then override with custom impl
- but if playing the sequence is easy?...

- sharing config is nice to have
- a sequence instantiates transactions or other sequences
- if a sequences needs to have swappable transaction types -> pass simple closure factory to the sequence

- factory for components and a factory for sequences and transactions
- factory has hierarchical overrides
- an overwrite at a certain level applies to all children (even if they define their own overwrites)
- configuration parameters are set at a certain level in the hierarchy and apply to all children
- again a childs definition of a parameter is overwritten by the parents definition
- the prioritization allows for child components to declare defaults

- params unique to one component can be passed as an argument to the factory
-
- a unique parameter configuration can be created using a builder pattern
- a builder can be created and new parameters can be added to it
- it can then be used to create components which will see those parameters

- a factory for each type (component, sequence, transaction) is necessary because they are configured differently
- a component needs to setup its children

- the config system is only to be used in the constructor phase
- communication between components is facilitated by channels which are easily passed around through the config system

\todo{should the factories for non-components be separate or merged?}

\end{comment}


\begin{listing}
\begin{lstlisting}[language=scala, captionpos=b, caption={Demonstration of the API for the four different factories for components, sequences, transactions and configuration objects.},label=lst:factory]
Config.set(key, value)
Config.get(key)

Config.get[MyConfig].myParam

Comp.overrideType[MyType, MyDerivedType]

Comp.create[MyComponent]
Sequence.create[MySequence]
Tx.create[MyTx]

Comp.create[MyComponent](
  key -> 10, ...
)

Comp.builder
  .withParams(
    "param1" -> 10,
    key -> "hello"
  )
  .withOverride[MyDriver, MyDerivedDriver]
  .create[MyComponent]

\end{lstlisting}
\end{listing}



\begin{comment}

  - config system:
- one is strictly hierarchical
- then there is a global one

- a component has a context inside its body
- components which are created can use this context to get their parent
- they can get the configuration of the parent
- the factory accepts a map of additional key-value pairs
- any factory overrides are also inherited to the children

- the factory can create a specific contex before creating a component!
- that means unique parameters and overrides

- the UVM uses factory to be able to exchange component implementations
- due to the standardized constructor interface which only has the name and parent for components, the config db is necessary to pass parameters to components
- the hieararchy awareness of the config db allows for parameters to be set for specific components
- but it also makes it more concise to share parameters between components, they only have to be set once with the correct path

- in this case program is assembled ONCE, and the code behavior changes by external configuration of factory
- this a very powerful concept, but it is not the only way to achieve configurability

- the use of the factory makes the passing of parameters circumvent the way the language facilitates it: through the constructor or functions
- another so-called creational pattern is dependency injection \cite{ioc_di}
- in dependency injection, the components are passed their dependencies through their constructor
- that means if a component wants to facilitate the use of a different sub-component, the OBJECT is provided from its parent
- either through a constructor or a setter method

- using dependency injection, swappable components are passed alongside parameters to a component through the constructor

- another way to facilitate configurability is through callbacks
- a callback class defines a set of functions
- the component checks for registered callbacks of the callback class and calls them
- the test code can register a callback implementation dynamically to change the behavior of the component
- according to \cite{sutherland2015uvm} this should be avoided
  - bad performance
  - convoluted to setup
  - OOP should be used instead

- in scala functions are first class citizens
- that means functions can be easily passed to components
- the components only need to provide an interface to exchange the function
- either callback function is provided in constructor or through a setter

- this is a form of dependency injection, just with the unit of dependency being a function instead of a component

- pros factory
  - easy to change out components
  - write the code flexible from the start

- cons
  - parameters are not directly passed in the constructor
  - they have to be fetched through the config db which requires error checking all the time

- pros dependency injection
  - parameters are passed directly to the constructor
  - the compiler can check if all parameters are provided

- cons
  - modularity has to be anticipated, the code has to be written in a way that components can be swapped out

- dependency injection does not require any infrastructure from the framework
- as such, it should be attempted to implement factory and config db in scala to see if they are easier to handle in the scala way

UVM requires a recompilation for each testbench \cite{salemi2013uvm}, we do not need that since a jar can have
multiple entry points.

\todo{consider callbacks -> in scala easy to implement with higher order functions}

- alternative to factory could be dependency injection
- an agent receives its driver through a parameter
- driver is of certain trait or class or a derivative thereof

- configDB should not require error handling all the time, throw exception on miss and have try method
- else parameters as class parameters for components
- causes problem with factory, since factory instantiation only accepts name, else varargs but that is not compile time checked

- static hierarchy, and we want to switch out components -> need to anticipate switch
- why not do dependency injection? then the compiler will help us guarantee that the switch works
- this causes problem with UVM

- if the factory is used, we could pass a small config db to the factory
- each component should then specify its parameters such that the factory checks automatically if all paramters are defined
\end{comment}

\section{Logging} %---------------------------------------------------------------------------------------------------------
\label{sec:logging}

For debugging puporses it is important to have a logging system. Here the UVM provides a powerful logging system
which tags messages by the component that has issued it. This allows for filtering of messages by component. In
addition the UVM provides verbosity levels to control how detailed the logging should be.

In the verification framework, a similar logging system should be available. The component base type should provide
functions for printing info, warning and error messages. In addition to tagging these messages with the components
hierarchical name, source file and line number information should be added to give a pointer to where the message was
issued in the code. The functionality of fatal errors in UVM is not necessary, since the Scala exception system can
be used instead.


\begin{comment}
  \section{Register Abstraction} %-----------------------------------------------------------------------------------------------

\todo{should we just drop this?}


\end{comment}

\section{Constrained-Random Stimuli Generation} %------------------------------------------------------------------------------
\label{sec:crv}

\begin{listing}
\begin{lstlisting}[language=scala, captionpos=b, caption=Outline of the syntax for random variables and constraints.,label=lst:crv_syntax]
class MyTx extends Transaction {
  val a = Rand[Int](0 until 10)
  val b = Rand[UInt](3.W)
  val c = Rand[MyEnum]

  val c0 = constraint {
    a < b
  }
  val c1 = constraint {
    iff (a > 5) { c === MyEnum.A }
  }
}
\end{lstlisting}
\end{listing}

The SystemVerilog language demonstrates how powerful the integration of random variables with constraints along side
a constraint solver into the language itself can be. A simple class definition with fields marked with the \ttt{rand}
keyword together with a set of constraints allows one to create a random stimulus stream. The ChiselVerify project showcases how a small domain-specific language for a
constraint system can be created in Scala. While it was decided that constrained-random stimulus generation was outside the scopt of this project, some general throughts onn how it could be integrated with the rest of the verification framework shalll be given. 

In ChiselVerify, only integer types bounded by a range are supported as random types. Instead, a \ttt{Rand[T]} type
should be introduced where \ttt{T} could be any scala type, also a random type itself allowing for nesting of
randomizable objects such as in SystemVerilog. It would have to be investigated whether existing constraint solvers
can handle all primitve Scala types such as floating points directly, otherwise they could be broken done further
into integer types for the sign, mantissa and exponent when the porblem description is handed to the solver.

In addition to primitive Scala types, bit-vector types where the number of bits limit the value range should also be
supported like in ChiselVerify. Inspiration for how to generate general Scala types could be taken from
property-based testing frameworks such as ScalaCheck \cite{scalacheck}, which automatically generate random test
cases for a given function. For the verification of singal processing circuits, it may be of interest to add
continuous random variables, which are only allowed a certain rate of change between two randomizations.

In the verification framework, random variables should be used in transactions. A simple sketch for a syntax is shown
in Listing \ref{lst:crv_syntax}.



\begin{comment}
  However, it only supports integer types with associated ranges.

- variable types
- support not only for hardware types but scala types
- bitvector types
- integer types
- floating point types
- enumeration types
- string type?
- randomize with constraints

- for signal processing purposes it may be interesting to have a continuous random signal i.e. a random walk

\end{comment}

\section{Functional Coverage} %---------------------------------------------------------------------------------------------

\begin{listing}
\begin{lstlisting}[language=scala, captionpos=b, caption=Outline of the syntax for the collection of functional coverage.,label=lst:cover_syntax]
class MyGroup extends CoverGroup {
  val pa = coverpoint(a) {
    bin("a0", 0 until 10)
    bin("a1", 10 until 20)
  }
  val pb = coverpoint(b) {
    bin("b0", 0 until 10)
    bin("b1", 10 until 20)
  }
}
\end{lstlisting}
\end{listing}

Functional coverage is a tool which must be provided by a modern verification framework. Without it, no realistic
picture of how well a developed test suite covers the features outlined in the specificaiton can be obtained.
SystemVerilog has its own language subset for functional coverage collection revolving around coverage groups. The
ChiselVerify project provides mechansims to express functional coverage but only on the inputs and ouputs of the DUT.
Although outside the scope of the project, a short outline of how functional coverage could be integrated into the
verification framework shall be given.

Functional coverage has to be measured on random variables and is therefore closely connected with the
constrained-random stimulus generation. Like in SystemVerilog and ChiselVerify, coverage collection should build on
the concepts of coverage groups, coverage points and coverage bins. A coverage point is specific to a random variable
and defines a different sets, the bins, which the random variable can take a value of. Coverage groups collect a set
of related coverage points. While the coverage groups should be classes such that they can be composed, coverage
points and bins should use a functional syntax as it is done in ChiselVerify. An outline of the syntax is given in
Listing \ref{lst:cover_syntax}. The \ttt{coverpoint} method should take a context parameter in order to register
itself with the coverage group. The same applies for the \ttt{bin} method. This allows for a concise declarative
syntax as in SystemVerilog. The cross-coverage feature should orient itself at the ChiselVerify implementation.

On important factor for coverage collection is when coverage should be sampled. Here it would be beneficial to
register sampling events with the simulation runtime. This would allow for the simulation runtime to detect the
sampling event instead of the user code having to manually trigger the sampling.

\begin{comment}
  Functional coverage information should be collectable for any scala type.

- functional coverage

- coverage groupps can be hieararchy aware!

functional coverage should be able to collect coverage for scala values! transactions are where coverage is measured

a covergroup with its sampling event should be passed to verification runtime
\end{comment}



\section{Debugging} %-------------------------------------------------------------------------------------------------------

While tests should be self-checking, it may be necessary to debug a failing test. In this case, waveforms are an
important tool which allow for the tracing back of the error to its source. Verilator supports the dumping of the
state of a model to VCD files. Since the recording of waveforms comes with an overhead, it should be optional to
enable it. As such, the file name for the waveform is accepted as an optional argument to the \ttt{Simulate} and
\ttt{runTest} functions.

Debugging purely using waveforms can be cumbersome. A better approach can sometimes be to debug the simulation
interactively. Debugging SystemVerilog testbenches has the advantage of a shared design and verification code base with one simulator executing everything. This allows for the insertion of breakpoints anywhere and stepping through code
in a debugger which is integrated into the simulator. Testing frameworks like Chiseltest and Cocotb do not allow for
this level of interactive debugging. 

While it is outside of the scope of the project, it could be of interest to
explore options of how interactive debugging might be achieved in co-simulation frameworks. One approach could
involve the Scala REPL, a shell where Scala code can be executed interactively. A new simulation entry point, like
shown in Listing \ref{lst:debug_syntax}, could be provided to directly return a reference to the DUT. This reference
could then be used to interact with the DUT in the Scala REPL. The waveform could be dumped each clock cycle such
that the user may inspect signals while the simulation evolves.

\begin{listing}
\begin{lstlisting}[language=scala, captionpos=b, caption=Outline of an interactive debugging approach using the Scala REPL.,label=lst:debug_syntax]
> val dut = simulateInteractive(new MyDut)
> val bfm = new MyBfm(dut)
> bfm.send(new MyTx)
> println(bfm.receive())
\end{lstlisting}
\end{listing}

\chapter{Implementation} %/////////////////////////////////////////////////////////////////////////////////////////////////////

In this chapter, some aspects of the implementation of the verification framework are discussed. The chapter is split into two parts, the first part discusses the implementation of the simulation interface which is provided by the simulation runtime. The second part discusses the implementation of the verification enviroment which is built on top of the simulation interface and provides the primitives for composing flexible testbenches.



\section{Simulation Runtime} %=================================================================================================

The simulation runtime consists of Scala library code which bridges the interaction between the test code and the Verilator model. This means that it has to provide an interface to multiple concurrent simulation threads, coordinate their execution and communication between them and translate the commands received by the simulation threads to interactions with the Verilator model of the DUT.

\subsection{Interfacing the Verilator Model} %---------------------------------------------------------------------------------

Verilator transpiles the SystemVerilog source code into a C++ class. A mechanism is needed by which the Scala test
code can interact with an instance of this class. The JNA (java native access) library can be used to call native
functions from the java virtual machine (JVM). In C++, a simulation context class is used to group all objects associated with an simulation instance. The following set of functions is provided, which take the simulation context as an argument to interact
with the simulation:

\todo{in the impl: name when inspiration was drawn from existing work}

\begin{itemize}
  \item \ttt{createSimContext\_XX} creates a new simulation context and return a pointer to it
  \item \ttt{destroySimContext\_XX} destroys a simulation context
  \item \ttt{setInput\_XX} sets the value of the input signal mapped to the provided id
  \item \ttt{setInputWide\_XX} sets the value of the input signal mapped to the provided id for inputs wider than 64 bits
  \item \ttt{getOutput\_XX} gets the value of the output signal mapped to the provided id
  \item \ttt{getOutputWide\_XX} gets the value of the output signal mapped to the provided id for outputs wider than 64 bits
  \item \ttt{tick\_XX} evaluates the model and advances the simulation by one clock cycle
  \item \ttt{getRegister\_XX} gets the value of the register mapped to the provided id
\end{itemize}

The functions are suffixed with the name of the top-level module (signified by \ttt{\_XX}) to make them unique,
allowing for multiple models to be loaded into the same JVM instance. The source code for these functions and the simulation context class is generated dynamically before a simulation is started. They are then
compiled together with the Verilator model into a shared library which can be loaded by the JNA library at runtime. A
Makefile is generated alongside the model specific simulation interface functions before a
simulation is started. The usage of a Makefile results in the shared library to be recompiled only when the model changes.

The inputs and outputs of a DUT are associated with a static id which allows for an efficient lookup of the signal in the C++ source code through a switch statement. Inputs and outputs wider than 64 bits are split into arrays of 32-bit integers in Verilator models. Here the simulation runtime has to take care of the splitting and reassembling of the values. Verilator provides access to internal state variable of the model. Since registers have to be declared statically as part of the interface of the DUT in Scala, they can also be associated with an id which is used to reference them in the C++ code.


\subsection{Concurrency} %----------------------------------------------------------------------------------------------

Scala progams are executed on the java virtual machine (JVM). Multiple threads can be created in the JVM which are
mapped to operating system threads. These threads are relatively expensive to allocate since they require their own
stack. In Cocotb, python coroutines are used, which offer a form of cooperative multitasking with very inexpensive
threads \cite{pycoroutines}. The JVM does not support coroutines. However, an effort exists to add green threads to
the JVM, i.e. threads which are managed by the language runtime instead of the operating system. Project Loom
\cite{loom} adds so-called virtual threads which are controlled by the JVM and dynamically mapped to a fixed pool of
operating system threads. These virtual threads are cheap to create and many can exist at the same time. This works
well for a simulation environment which may spawn many threads with a short life time which handle a small amount of
work. The virtual threads provided by project loom are included in Java 21 and later verions.

A new library for Scala 3 called Gears \cite{gears} has been developed by researchers at EPFL, which tries to take
advantage of virtual threads to facilitate asynchronous programming, i.e. programming where computations are
encapuslated into so-called \ttt{Future[T]} objects. A computation inside a future will run concurrently, but can be
\textit{awaited} by another thread of execution, causing it to block until the computation is finished. This allows
for a more structured concurrency model than the traditional purely thread-based model. It is well suited for the
fork-join model which was decided to be used in the verification framework, since a fork is essentially a future. In
addition to a basic future type, the Gears framework also provides channel primitives which suspend the executing
virtual thread when waiting for a receiver or sender. This provides a good basis for implementing the channels which
should be used for communication between components in a testbench. It is therefore decided to use the primitives
provided by the Gears library to build the fork-join model in the verification framework.

\begin{comment}
In the verification framework, the future type is wrapped by a \ttt{Fork[T]} type

- threads are an option but have a lot of overhead, each their own stack
- in cocotb coroutines are used
- project loom adds green threads to the JVM \cite{loom}
- a new library in scala 3 by epfl called gears tries to take advantage for async style programming \cite{gears}
- uses scalas context parameters: functions use context dependent values
- async functions take async context as parameter def myfun(...)(using Async)
- this allows for structured concurrency -> a newly spawned thread knows about its parent
- well suited for fork-join model

\end{comment}

\subsection{Simulation Controller} %-------------------------------------------------------------------------------------------

In a multi-threaded simulation environment, interactions with the simulation have to be serialized. This could either
be achieved by requiring a thread to aquire a lock before performing its operation on the simulation or it could be
achieved through a message passing system, where a simulation controller receives commands from the simulation
threads through a channel. The latter approach was chosen, since it provides a clear separation of concerns:
Simulation threads only issue commands to the simulation controller and wait for a response. The simulation
controller handles incoming commands until all simulation threads are asleep, at which point it advances the
simulation, driving clock and input signals at the appropriate time. It is also the simulation controller which is
responsible for waking up threads in the clock cycle they requested through their \ttt{step} command.

Commands are sent through one channnel to the controller, which serializes them. Responses, should they be required,
are sent through a channel unqiue to the requesting simulation thread. This channel is provided to the simulation
controller when the thread registers itself with the controller. The registration is also necessary for the
controller to keep track of the status of all simulation threads. This allows it to know when all simmulation threads
have performed their work for the current clock cycle.

The channels which a simulation thread uses to issue commands and receive responses are the ones provided by the
Gears library. This channel primitive can however not be used directly to facilitate communication between
components. A thread which waits for a value on the channel does not inform the simulation controller that it is
sleeping and that the simulation may therefore advance. For this reason a wrapper around the channel is provided
which coordinates the blocking of the thread with the simulation controller. This custom type of channel shall from
now on be plainly referred to as a channel while the channel from the gears library shall be referred to as a gears channel.

A simulation thread can issue the following commands to the simulation controller:

\begin{itemize}
  \item \ttt{Register}: Make controller aware of the thread and pass response gears channel
  \item \ttt{Deregister}: Remove thread from controller
  \item \ttt{Poke}: Set the value of an input
  \item \ttt{Peek}: Get the value of an input or output
  \item \ttt{Step}: Request to be waken up \textit{n} clock cycles in the future
  \item \ttt{SendToChannel}: Send a value to a channel and potentially block
  \item \ttt{WaitForChannel}: Wait for a value on a channel and potentially block
  \item \ttt{WaitForThread}: Wait for a thread to finish
  \item \ttt{Finish}: Finish the simulation
  \item \ttt{Abort}: Abort the simulation with an exception
\end{itemize}

The \ttt{peek} command uses the response gears channel to receive a value. The \ttt{Step}, \ttt{SendToChannel},
\ttt{WaitForChannel} and \ttt{WaitForThread} commands use the response gears channel to block until the simulation
controller unblocks them by sending a dummy message.

While the simulation threads are running, the simulation controller is collecting a series of actions which it keeps
in an event queue. \ttt{Poke} commands create \ttt{Drive} events which are scheduled for the falling edge of the
clock domain of the signal which is being poked. A \ttt{Step} command schedules a \ttt{Release} event for the
requesting thread at the appropriate time in the future. Clocks are generated by \ttt{PosEdge} and \ttt{NegEdge}
events, which schedule each other with half-period offsets.

Once all simulation threads are marked as sleeping, either through stepping or blocking, the simulation controller
advances to the next time slot with events and executes them. The priority order between multiple events in the same
time slot is such that all interactions with the simulation (i.e. \ttt{Drive}, \ttt{PosEdge} and \ttt{NegEdge})
happen before threads are released.


All simulation threads require a reference to the simulation controller. This could be achieved by using a singleton
object, but this would only allow for a single simulation to run at a time. A so-called dynamic variable offers a solution to this problem. It allows to change the value from the view point of code run inside a certain context. 
A third option is the use of context parameters in Scala. In addition to the usual parameters, Scala functions and constructors can accept parameters which are passed \textit{implicitly} by the context of the caller. A context parameter is declared by adding \ttt{using} statement to function parameters, e.g. \ttt{def fun(..)(using T)}. In this case Scala will look for a context parameter of type \ttt{T} when \ttt{fun} is called. It can be provided by \ttt{given T = ...} in the calling context or it may inherit the context parameter from its caller. The Gears library uses a context parameter to pass a \ttt{Async} paramter around which keeps track of the current context which for instance includes all spawned futures. Similarly, a simulation context could be used to keep track of the simulation threads and give access to the simulation instance.

Since Gears already relies on context parameters, it is decided to use the same mechanism to provide access to the simulation controller. As such functions which interact with the simulation controller need to have the
following signature: \ttt{def fun(args...)(using Sim, Async)}. Since interactions with the simulation controller require the use of the Gears library due to the usage of channels, the \ttt{Async}
context parameter is also required. The functions to interact with the DUT such as \ttt{dut.port.poke(value)} use the
\ttt{Sim} and \ttt{Async} context parameters to send the appropriate command to the simulation controller.

\begin{comment}

- accesses to the simulation have to be serialized
- either lock for simulation
- but who has responsibility to drive the simulation after all threads sleep?
- could be the last thread which sleeps
- instead a model is chosen where a simulation controller runs and receives commands from the simulation threads
through a channel
- channel serializes commands
- simulation threads issue request and wait to be waken up or receive an answer by waiting for a response on another channel
- commands:
- register thread
- deregister thread
- poke
- peek
- step
- Mark
- SentToChannel
- WaitForChannel
- WaitForThread
- finish
- Abort

- sim controller needs to know all simulation threads and keep track of their status
- therefore register and deregister commands are necessary

- sim controller keeps an event queue
- the following events exist
- Drive
- PosEdge
- NegEdge
- release

- pokes create a drive event at the falling edge of their clock domain
- peeks are served immediately
- clock events schedule the next edge when they are executed

- step command schedules a release event for the appropriate time
- once all threads sleep, the controller advances to the next time with an event, executes all events
- if threads are released, the controller waits for thm to finish before advancing to the next time

- since channel communication may put threads to sleep while waiting (i.e. sim is allowed to advance) the channel
communication has to happen through the sim controller
- join mechanism is the same

- finish command for good exit
- abort command for bad exit

- question is now how to give threads access to sim controller
- global means only a single sim instance
- dynamic variable would be solution
- but since Async uses context parameters -> Sim context parameter is used
- as such function which use the simulation controller def myfun(...)(using Sim, Async)
- async is needed since interaction with sim may put thread to sleep

- the actual functions to interact with the simulation are extensions to the port types
- they use the Sim context to send the appropriate command to the simulation controller
- example in Listing \ref{lst:simctx}
- simcontext is provided in sim closure as context parameters
\end{comment}

\begin{listing}
\begin{lstlisting}[language=scala, captionpos=b, caption=Example code for a function using the simulation context to interact with a DUT.,label=lst:simctx]
def add(dut: Adder)(a: Int, b: Int)
                   (using Sim, Async): Unit = {
  dut.a.poke(a)
  dut.b.poke(b)
  dut.clk.step(1)
  dut.c.expect(a + b)
}

Simulation(new Adder) { dut =>
  add(dut)(1, 2)
}
\end{lstlisting}
\end{listing}

\section{Verification Environment} %===========================================================================================

\subsection{Phases} %----------------------------------------------------------------------------------------------------------

The phasing system of the verification framework is an opt-in system. The component base type itself does not run in
any phases. Instead, traits for different phases can be mixed into the definition of a custom component. These traits
with their respective methods are:

\begin{itemize}
  \item \ttt{SimulationPhase}: \\ \ttt{def sim()(using Sim, Async.Spawn): Unit}
  \item \ttt{ResetPhase}: \\ \ttt{def reset()(using Sim, Async.Spawn): Unit}
  \item \ttt{ConfigPhase}: \\ \ttt{def config()(using Sim, Async.Spawn): Unit}
  \item \ttt{TestPhase}: \\ \ttt{def test()(using Sim, Async.Spawn): Unit}
  \item \ttt{ShutdownPhase}: \\ \ttt{def shutdown()(using Sim, Async.Spawn): Unit}
  \item \ttt{ReportPhase}: \\ \ttt{def report(): Unit}
\end{itemize}

As it can be seen, all phases except the report phase receive the simulation and async context parameters. The async
context parameter comes in a special form \ttt{Async.Spawn} which provides the capabilty to create new forks. The
report phase does not require the context parameters since it does not interact with the simulation. The phases are
invoked by the \ttt{runTest} function. Whether a component implements a certain phase is determined by a pattern
match on the trait.

\begin{comment}
- like it was discussed earlier, phases are implemented as traits
- a component can implement multiple phases by extending multiple traits
- each trait comes with one function which is called in the phase
- a function taking the root component as argument exists for every phase
- it starts up forks for simulation phases and else just calls the function
- whether a component implements a phase is easily determined by a pattern match on the trait
- inside the runTest the phases are called in order on the root components

\end{comment}

\subsection{Components} %-----------------------------------------------------------------------------------

The implementation of a standard library of components only involves extending the base Component type and adding the
necessary phases alongside some component specific functionality. Standard components such as drivers or monitors are
abstract classes, which means that they do not have to provide an implementation for their phasing methods.

The driver is a component implementing the simulation phase with two type parameters \ttt{A} and \ttt{B} which are
both transactions. \ttt{A} is the transaction which is sent to the driver, while \ttt{B} is its response for the
sequence. It has a special port \ttt{DriverPort[A,B]} which has a \ttt{ReceiverPort[A]} and a \ttt{SenderPort[B]}. It
furthermore provides methods to abstract the port iteractions: Calling \ttt{next()} returns the next item to drive,
while calling \ttt{respond(b)} sends the response to the sequence. It keeps track of the number of received
transactions, which can be queried through \ttt{numOfDrivenTxs}.

The default sequencer implementation also runs in the simulation phase. It also has a special port, matching that of
the driver with opposite direction: \ttt{SequencerPort[A,B]}. This port can be connected to a driver port. The
sequencer has a channel via which it receives sequences. It has a \ttt{def play(s: Sequence[T])} method which
abstracts the channel interaction for the test case writer. It provides a simulation phase implementation where it
loops, waiting for a new sequence and forwarding all its transactions to the driver.

The monitor is a component implementing the simulation phase with one type parameter \ttt{T}. It allows for
connecting analysis components through the \ttt{def addListener(l: ReceiverPort[T])} method. It provides the
\ttt{publish(t)} method to abstract the sending of transactions to all listeners. The monitor automatically keeps
track of the number of observed transactions, which can be queried through \ttt{numOfObservedTxs}.

An analysis component implements the simulation phase and has a receiver port. This port should be connected to a
monitor. New items can be awaited using the \ttt{next()} method. A special variant of the analysis component is the
scoreboard which provides not further functionality.

\subsection{Stimulus and Sequences} %-------------------------------------------------------------------------------------------

Stimulus is represented by the \ttt{Transaction} type. For now, this type is completely empty, providing no fields or
methods. Future extensions could add features such as transactions ID's and the constrained-randomization features
discussed in Section \ref{sec:crv}.

Sequences are implemented using channels which are hidden from the user. The user implements the sequence in the
\ttt{body} method using the \ttt{yieldTx(t)} and \ttt{yieldSeq(seq)} methods. These methods actually send
transactions through one channel and await a response on another. Creating an instance of the sequence spawns a
separate fork for it, in which the body is run. The channels in the sequence are used by the sequencer to get
transactions and forward them to the driver, as well as send back the response from the driver. One can wait for a
sequence to finish by calling \ttt{waitUntilDone} which awaits the fork executing the body.

\todo{not all composition methods have been implemented}

\subsection{Test Cases} %------------------------------------------------------------------------------------------------------

The test case is the root component of a testbench. It should receive a reference to the DUT or a BFM through its
constructor. The \ttt{Test} base class is a component which includes the \ttt{TestPhase} without providing an
implementation. Further phases can be freely added by extending the respective traits. In the current implementation,
each test case class has to be paired with a \ttt{runTest} function to actually run the test. Since a Scala program
can have multiple entry points, each test can be associated with its own entry point by annotating the function
calling the test with the \ttt{@main} annotation.

\subsection{Factories and Configuration} %-------------------------------------------------------------------------------------

\todo{formulate}

- needs to be hiearchy aware
- this is done through Hierarchy context parameter which all components, sequences, and txs have to accept
- this is assumed by the factory when creating instances
- the hierarchy holds references to parent and local params and overrides
- a factory call and parameter look up goes through the hierarchy to find the highest level defintion
- factory itself is done through ClassTags with allow for obtaining constructor at runtime and creating instances

\chapter{Evaluation}
%////////////////////////////////////////////////////////////////////////////////////////////////////////////

The proposed verification framework is evaluated by applying its implementation to four different use cases. The use
case demonstrate different levels of complexities and features of the verification framework. The first use case
demonstrates the basic functionality of the verification framework by testing a design in a directed manner using a
BFM. It also shows how internal registers may be read. The second use case demonstrates how the verification
framework handles multiple clock domains and how a concurrent testbench can be written. The third use case replicates
a UVM-style testbench which the PyUVM authors use as an example, allowing for a comparison of the implementation in
Scala and Python. The fourth use case replicates verification blocks for an APB interface which are implemented in
PyUVM and serve as verification infrastructure for the Didactic SoC mutli-project chip developed as part of the
Edu4Chip project \cite{edu4chip}.

\todo{since focus is on improving usability and showcasing use of modern lang -> no performance eval}

\section{Use Case 1: Greatest Common Divisor} %--------------------------------------------------------------------------------

The first use case is a simple greatest common divisor (GCD) circuit, which is taken from the lab work of the DTU
course \textit{02203 Design of Digital Systems}. This use case should demonstrate the writing of simple directed test
cases and the use of BFM's in the verification framework. It is also used as a small showcase for the reading of
internal registers. The design is interfaced via a 4-phase handshake protocol with a request and acknowledge signal.
The two operands are sent one after the other, and the circuit acknowledges the second operand with the result of the
calculation. The implementation of the circuit is done in SystemVerilog.

The GCD design is wrapped into a BFM in the test code. The BFM provides methods to reset the design, to drive one
handshake and to perform the two handshakes required to calculate the GCD of two numbers while checking that the
result is correct. Furthermore, it provides a method which prints the value of the state and operand registers. The
test code is shown in Listing \ref{lst:gcd_test} in the appendix. A simulation with a resolution of \SI{1}{ns} is
created which dumps waveforms to \ttt{gcd.vcd}. The BFM is used to drive the transactions and check the result
automatically. A fork is created which prints the state of the DUT in every clock cycle. A waveform showing the
handshake for the first transaction is shown in Figure \ref{fig:gcd_timing}.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{diagrams/gcd_timing.pdf}
  \caption{Wavefrom showing the handshake for the calculation of the greatest common divisor of 12 and 3.}
  \label{fig:gcd_timing}
\end{figure}

\section{Use Case 2: Clock Domain Crossing} %-------------------------------------------------------------------------------

The second use case should demonstrate how a design with multiple clocks is handled. For this purpose a primitive
clock domain crossing circuit is used. The circuit uses a 4-phase handshake with the request and acknowledge signals
crossing the clock domains through two synchronization flip-flops from a \SI{6}{ns} clock to a \SI{10}{ns} clock. The
receiving clock domain is allowed to sample the data provided by the sending clock domain in the time window between
request being asserted and acknowledge being asserted. The design is implemented in SystemVerilog.

In the test code, two threads are spawned, one for each clock domain. The sender side resets its synchronization
flip-flops and performs a handshake to transfer the value \ttt{0xDEADBEEF}. Both actions are performed through a BFM.
The receiver thread also resets its flip-flops and then takes part in a handshake, checking whether the sampled value
is the one that is expected. The test code is shown in Listing \ref{lst:cdc_test} in the appendix, while a waveform
for the test is presented in Figure \ref{fig:cdc_timing}.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{diagrams/cdc_timing.pdf}
  \caption{Wavefrom showing the handshakes for the clock domain crossing of the value \ttt{0xDEADBEEF}.}
  \label{fig:cdc_timing}
\end{figure}

\section{Use Case 3: Simple ALU} %------------------------------------------------------------------------------------------

The third use case is used to compare the verification framework with PyUVM by replicating an example from the PyUVM
project which tests a small ALU design \cite{pyuvm_tinyalu}. The ALU design supports single cycle AND, XOR and
addition operations, as well as multi-cycle multiplication. The SystemVerilog implementation from the PyUVM repository is used.

The testbench in PyUVM follows the standard UVM structure, except for not using the agent component. The driver and
monitor are placed directly in the environment and interact with the DUT, while a scoreboard and coverage collector
analyze the transactions observed by the monitor. The scoreboard checks the results of the ALU with a simple
predictor. The coverage collector simply checks whether all operations have been performed.

A small library of sequences is provided. A random sequence produces one transcation per operation with random
operands. A max sequence produces a transaction for each operation with \ttt{0xFF} as the operands. Finally, a
fibonacci sequence sends a transactions to calculate the fibonacci sequence up to the 10th number. These sequences
are combined in two merged sequences: one playing them in series, the other one paying them in parallel.

This testbench setup is replicated in the verification framework to match the PyUVM testbench's capabilities as
closely as possible. This allows for a comparison of the two testbenches in terms of their complexity. Measuring
complexity at the source code level is a diffifult task and can only provide a rough direction of how effective one
framework is over the other. On the one hand, each component can be investigated individually. Table
\ref{tab:comparison} shows how many methods a component has to implement to achieve its functionality and how many
statements the code inside the component contains. The first number for each component is the count for the PyUVM
framework while the second is the count for the designed verification framework. Statements are counted roughly as
assignments, function calls or control-flow statements. The proposed verification framework requires generally fewer
statements and an equal or lower number of methods to achieve its functionality.

Another way to compare the complexity of the two testbenches is to simply count the number of lines of code,
excluding empty lines of course. The PyUVM testbench is split into two files, one containing a BFM and the other the
testbench itself. Combined they contain 334 lines of code. The scala testbench on the other hand contains 276 lines
of code. Lines of code only gives a rough estimate, since complex expressions could be merged into a single line
leading to a lower line count but the same complexity. Ideally, a more sophisticated metric would consider the tokens
in the code. An approximation can be made by counting the amount of whitespace separated tokens in the code. The
PyUVM testbench contains 1024 of these tokens, while the scala testbench contains 877 tokens.

\begin{table}
  \centering
  \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Component}       & \textbf{Methods} & \textbf{Statements} \\ \hline
    Driver          & 4/1             & 15/7 \\ \hline
    Monitor         & 3/1             & 8/5    \\ \hline
    Scoreboard       & 3/2            & 28/14    \\ \hline
    Coverage      & 3/2             & 14/9      \\ \hline
    Environment        & 2/0             & 10/7    \\ \hline
    RandomSeq   & 1/1 & 5/4 \\ \hline
    MaxSeq   & 1/1 & 4/2 \\ \hline
    FibonacciSeq   & 1/1 & 10/7 \\ \hline
  \end{tabular}
  \caption{Comparison of the method and statement count between the PyUVM testbench and the verification framework
    testbench for the TinyALU design. The first number in each cell is the count for the PyUVM testbench, the second
  number is the count for the verification framework testbench.}
  \label{tab:comparison}
\end{table}

% testbench.py 229 (wc 702)
% tinyalu_utils.py 105 (wc 322)
% total 334 1024

% TinyAlu.scala 276 (wc 877)

\todo{parallel seq is not working for now}

\section{Use Case 4: APB Verification IP} %---------------------------------------------------------------------------------

\todo{compare lines of code roughly, is there a difference in design components? what was not replicated, what was
replicated differently?}

\cite{didactic}

\chapter{Discussion} %/////////////////////////////////////////////////////////////////////////////////////////////////////////

%1. rough summary of what was done
This thesis has proposed a verification framework for SystemVerilog designs which is implemented in Scala 3. The framework allows for multi-threaded testbenches using a peek/poke/step interface to interact with DUT. On top of the simulation interface which facilitates the construction of unit tests, a library of standard testbench components taken from the UVM was implemented. The execution of these testbenches is structured through a simplified phasing model. Configuration and flexibility is provided through the use of compile time safe factories for components, sequences, transactions and configuration objects alongside a hierarchy aware parameter system inspired by the UVM factory and configuration database.

%2. go through features, compare them to existing solutions
  % - highlight strengths and explain limitations 
  % - name how they could be improved
  % - include evaluation use cases

The frameworks primary interface to the simulation is influenced by the ChiselTest framework. 
- uses step to advance time 
- this is less flexible than what is possible in SV and cocotb but works well for sequential designs which are the most common case
- it is thus impossible to directly synchronize with edges, but code can only ever run on the falling edge of the clock
- one issue in this framework which chiseltest does not have is that mealy outputs are not working correctly, since the change in value is first visible to the testbench after the next clock edge because inputs are driven after sim threads have finished work
- this should be fixed
- one feature which the framework improved is the support for multiple clocks
- this is done through the use of a clock domain which groups ports with a clock to decide their drive times
- clocks can have periods and the simulation resolution can be controlled

%multi threading
- the framework uses virtual threads in the JVM over normal ones in Chiseltest
- furthermore chiseltest even though allowing multithreaded tbs actually executes on thread after the other -> not true parallelism
- cocotb's coroutines are also not true parallelism but cooperative multitasking
- this framework uses true parallelism through virtual threads
- the gears library was used to provide a set of concurrent primitives such as futures and channels
- this is not strictly necessary and a more direct integration with the simulation environment would make sense in the long run
- especially since unlike in cocotb, the actual asynchronous operations are abstracted away (i.e. no await calls in tb code)

% context params
- the usage of gears requires the Async context parameter to be added to all functions which interact with the simulation
- on top of that the Sim context parameter is used to provide access to the simulation controller
- this is a bit cumbersome
- if gears was removed, the Async context parameter could be removed as well
- else it may be worth considering to use dynamic variables to provide access to the simulation controller such as it is done in chiseltest
- context parameters is however the prefered way in scala 3 to provide context dependent values which a simulation conext is
- in chiseltest, calling some methods without the sim context being defined causes runtime errors whereas in this framework it is a compile time error

% interfacing with Sv
- the RTL design is simulated using a Verilator model
- like in chiseltest, native functions interacting with the C++ model are called through JNA
- one advantage that this simulation approach has, which chiseltest and cocotb share, over SV is that simulation model compilation is decoupled from tb compilation
- this means a change in tb does not require a recompilation of the RTL
- this is especially useful for large designs
- in the framework, the control of the simulation happens in Scala -> therfore many native calls which are expensive
- in spinal HDL sim, the control is done in C++ which calls the scala code
- it would be interesting to see how this affects performance
- what should be done is to limit the number of calls to the native code
  - set multiple inputs at once

% unit tests
- small unit tests are possible through the Simulation entry point
- chiseltest integrates tests with the scalatest framework to organize test execution
- tests are declared with a text and are grouped together
- this is not possible in the framework but could be added in the future

% complex TB
- the framework ended up orienting itself closely on UVM
- the UVM and especially the subset presented by \citeauthor{sutherland2015uvm} provides a clear and relatively simple structure for testbenches
- it allows to ignore many of the more complex features of the UVM
- the structure of components, sequences and test cases establishes clear interfaces between the different parts of the testbench and makes sense
- what this framework provides is some simplifications in the conccepts and some adjustments to the API to make the usage more concise and simple
- furthermore, it was aimed to make the usage more safe through scalas type system


%3. collect future work and prioritize different tasks







1. summary of findings
- recap results from design and implementation
- relate results back to original research questions

- a framework to verify systemverilog designs in scala 3 was designed and implemented
- the framework allows for multi-threaded testbenches using fork-join model on top of lightweight virtual threads
- interaction with the dut is doen through a peek/poke/step interface
- internal registers can be read 
- multiple clock domains are supported
- simulation uses verilator as a backend
- testcase is purely scala code -> interface of DUT + actual test code

- a library of standard testbench which follow UVM concepts was implemented
- the execution of testbench is structured by a simplified phasing system of UVM
- the coordination of phasing is simplified by seperating service providing components and controlling components, i.e. the test case
- testbench primitives which facilitate synchronization
- channels between components through ports which are integrated with simulation runtime
- generator-like sequences alongside scala sequences for testbench

- compile time safe factories for components, sequences, transactions and configuration objects
- parameter passing through hierarchy context  


- a statically type safe factory override interface was implemented
- creating instances can still fail however due to constructor mismatch

2. comparison to existing work
- how does the framework compare to existing verification frameworks and methods
- does it improve efficiency, usability, scalability?

- framework orients itself closely on existing work such as Chiseltest, cocotb and UVM
- but provides a series of small improvements over these

- the framework follows the step abstraction of chiseltest but extends it to multiple clocks
- 

3. strengths and contributions
- what aspects work particularly well and are novel?
- how does the framework improve certain aspects?

- decoupling of compiling simulation and testbench (only advantage if no incremental compilation for SV is available)
- all test code only in scala -> not using makefile setup like in cocotb
- direct integration of SV unlike in Chiseltest where SV designs have to use a chisel wrapper

- in the end framework orients itself quite closely to UVM and works more at the API level than at the methodology level
- phasing is opt-in and abstract classes allow forcing derived classes to implement certain phases

- a statically safe factory 

4. limitations and challenges
- what are the main difficulties and limitations of the framework?
- are there cases where it does not work as expected?

- factory does not allow for overrides from CLI -> this is largely motivated by single entry point in SV -> here one entry point per test case 

- there usage of context parameters should be re-evaluated, at least Async should be part of Sim

- it should be possible to use mealy outputs in the framework, i.e. pokes should drive immediately while placed at the falling edge
- this way peeks can reflect the state of the DUT at the current time

5. future work
- what are the next steps for the framework?
- how can it be improved and extended?

- performance evaluation




- general topics
- what works well?
- what is not possible which is possible in UVM
- what could in general be improved


- contributions:
  - step system with multiple clocks
  - register access
  - uvm 
    - declarative opt-in phases through traits
    - removal of multiple build phases
    - reduced class library complexity (some of which should be hidden from the user anyways)
    - mixing UVM generator like seq with scala seq

use cases:
- less verbose -> we define behavior in one place and it is not spread over multiple

- scala 3 was chosen to use newest features and make future proof but that prevents integration with chisel

- annoying to write interface in scala and SystemVerilog -> simple tool could create scala code from SystemVerilog

- it is really difficult working with something which is promised to bear fruit for really complex cases when time is
limited and does not allow for creating cases which would show the full potential
- I also don't have the use case and expertise to showcase full blown UVM
- the design of such a framework should build on actual experience with large verification projects


- it could be useful to have a user extendible phasing system


- register access should be typed and include writing

- it is hard to motivate a framework made for reuse with simple use cases
- but complex use cases are outside the scope of the project

- performance => we should implement more in C++ and cross the native jvm boundary less often
- we can hand references to closures to native code and it can invoke callbacks -> this is how cocotb does it

- sequences can not send next item before receiving response
- for pipeline interfaces this is not good

- the framework simplefies some things, how can we be sure that this does not create an issue for more complex cases?
- we are aiming to provide a framework which is easy to use for common case
- is it then still universal?

%\section{Future Work} %========================================================================================================



- verilator performance features (multi-threading, ...)

- allow for setting top level verilog parameters

- add support for attached models
- a function receiving the dut and can do whatever it likes, reading outputs, setting inputs to for instance model a
UART/SPI/Memory
- simulation loop could call the function in each evaluation step

- integration with scalatest
